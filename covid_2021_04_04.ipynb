{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe55404",
   "metadata": {},
   "source": [
    "# Ulepszony Model SIRD z Mechanizmami Redukcji Residuów i Prognozą Stochastyczną"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03da7c6",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Notebook ten implementuje model epidemiologiczny **SIRD** dla danych dotyczących epidemii Covid-19 w Polsce. Głównym celem projektu jest:\n",
    "\n",
    "1.  **Dopasowanie parametrów:** Wykorzystanie PSO z akceleracją GPU, próbkowaniem Latin Hypercube (LHS), adaptacyjną bezwładnością i strategią wysp.\n",
    "2.  **Korekcja residuów:** Zastosowanie zaawansowanych technik do modelowania i prognozowania błędów (residuów) modelu SIRD, w tym:\n",
    "    *   Modele Transformer (sieci neuronowe z mechanizmem uwagi).\n",
    "    *   Ulepszone Łańcuchy Markowa (z pamięcią i cechami czasowymi).\n",
    "    *   Model hybrydowy łączący obie powyższe techniki.\n",
    "3.  **Analiza stochastyczna:** Generowanie wielu symulacji w celu uchwycenia niepewności:\n",
    "    *   **Bootstrapping parametrów:** Estymacja rozkładu parametrów modelu.\n",
    "    *   **Generowanie zróżnicowanych parametrów:** Użycie Kernel Density Estimation (KDE) do próbkowania z estymowanego rozkładu.\n",
    "    *   **Generowanie symulacji stochastycznych:** Uruchamianie modelu z różnymi zestawami parametrów.\n",
    "4.  **Adaptacyjne przedziały ufności:** Tworzenie bardziej realistycznych przedziałów ufności uwzględniających błędy dopasowania i zmienność procesu.\n",
    "5.  **Wizualizacja:** Czytelne wykresy przedstawiające medianę prognozy wraz z przedziałami ufności.\n",
    "\n",
    "**Cel:** Uzyskanie dokładniejszych prognoz rozwoju epidemii oraz lepsze oszacowanie niepewności tych prognoz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdc8ea",
   "metadata": {},
   "source": [
    "## 1. Setup: Import biblitoek, definicja parametrów globalnych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b94126",
   "metadata": {},
   "source": [
    "### 1.1 Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from numba import cuda\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import multivariate_normal\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import qmc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74fa12",
   "metadata": {},
   "source": [
    "### 1.2 Parametry globalne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36a477",
   "metadata": {},
   "source": [
    "brak parametru substeps -> korzystamy z RK4, nie Eulera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 150\n",
    "NUM_PARTICLES = 10_000\n",
    "DT = 0.5\n",
    "POPULATION = 38.2e6\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e979b",
   "metadata": {},
   "source": [
    "## 2. Ładowanie i przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_covid_data(csv_path=\"data/preprocessed_Poland.csv\"):\n",
    "    \"\"\"Wczytuje dane COVID-19. (Poprawiono formatowanie)\"\"\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"Last_Update\"])\n",
    "    df = df.sort_values(\"Last_Update\")\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e98c5c",
   "metadata": {},
   "source": [
    "## 3. Definicje modelii, komponentow podstawowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3845d",
   "metadata": {},
   "source": [
    "### 3.1 Symulacja RK4, funkcja Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_beta_function(day, t1, t2, beta1, beta2, forecast_start_day=None,\n",
    "                          start_date=None, forecast_days=0, seasonal_factor=0.15):\n",
    "    if day < t1:\n",
    "        beta_base = beta1\n",
    "    elif day < t2:\n",
    "        frac = (day - t1) / max(t2 - t1, 1e-8)\n",
    "        beta_base = beta1 + frac * (beta2 - beta1)\n",
    "    else:\n",
    "        beta_base = beta2\n",
    "    if forecast_start_day is not None and day >= forecast_start_day:\n",
    "        days_in_fc = day - forecast_start_day\n",
    "        decay = 0.98 ** (days_in_fc / 7)\n",
    "        beta_base *= decay\n",
    "    if start_date is not None:\n",
    "        curr_date = start_date + pd.Timedelta(days=day)\n",
    "        doy = curr_date.dayofyear\n",
    "        season = seasonal_factor * np.cos((doy - 15) * 2 * np.pi / 365)\n",
    "        beta_base *= (1.0 + season)\n",
    "    return beta_base\n",
    "def enhanced_simulate_sird_rk4(params, days, S0, I0, R0, D0, dt=DT, Npop=POPULATION,\n",
    "                           forecast_start_day=None, forecast_feedback=True, start_date=None):\n",
    "    beta1, beta2, t1, t2, gamma_base, mu_base = params[\"beta1\"], params[\"beta2\"], params[\"t1\"], params[\"t2\"], params[\"gamma\"], params[\"mu\"]\n",
    "    S_arr, I_arr, R_arr, D_arr = (np.zeros(days) for _ in range(4))\n",
    "    S, I, R, D = S0, I0, R0, D0\n",
    "    corr_factor = 1.0\n",
    "    delay = 5\n",
    "    I_hist = np.zeros(delay)\n",
    "    I_hist[0] = I0\n",
    "\n",
    "    def sird_derivs(S_loc, I_loc, R_loc, D_loc, day_idx, substep_frac):\n",
    "        curr_day = day_idx + substep_frac\n",
    "        avg_I = np.mean(I_hist)\n",
    "        gamma_eff = gamma_base * (1.0 + 0.1 * (avg_I / max(I_loc, 1e-6) - 1))\n",
    "        mu_eff = mu_base * (1.0 + 0.05 * (avg_I / max(I_loc, 1e-6) - 1))\n",
    "        gamma_eff = max(0.8 * gamma_base, min(1.2 * gamma_base, gamma_eff))\n",
    "        mu_eff = max(0.8 * mu_base, min(1.2 * mu_base, mu_eff))\n",
    "        beta_t = seasonal_beta_function(curr_day, t1, t2, beta1, beta2, forecast_start_day, start_date, seasonal_factor=0.25)\n",
    "        if forecast_start_day is not None and curr_day >= forecast_start_day:\n",
    "            beta_t *= corr_factor\n",
    "        dS = -beta_t * (S_loc * I_loc / Npop)\n",
    "        dI = beta_t * (S_loc * I_loc / Npop) - (gamma_eff + mu_eff) * I_loc\n",
    "        dR = gamma_eff * I_loc\n",
    "        dD = mu_eff * I_loc\n",
    "        return dS, dI, dR, dD\n",
    "\n",
    "    for day in range(days):\n",
    "        S_arr[day] = S\n",
    "        I_arr[day] = I\n",
    "        R_arr[day] = R\n",
    "        D_arr[day] = D\n",
    "        I_hist = np.roll(I_hist, 1)\n",
    "        I_hist[0] = I\n",
    "        if forecast_feedback and forecast_start_day is not None and day >= forecast_start_day:\n",
    "            days_in_fc = day - forecast_start_day\n",
    "            if days_in_fc > 0 and days_in_fc % 2 == 0:\n",
    "                growth = (I - I_arr[max(0, day-2)]) / max(I_arr[max(0, day-2)], 1)\n",
    "                if growth > 0.03:\n",
    "                    corr_factor *= 0.93\n",
    "                elif growth < -0.03:\n",
    "                    corr_factor = min(1.0, corr_factor * 1.03)\n",
    "\n",
    "        k1_S, k1_I, k1_R, k1_D = sird_derivs(S, I, R, D, day, 0.0)\n",
    "        k2_S, k2_I, k2_R, k2_D = sird_derivs(S + 0.5*dt*k1_S, I + 0.5*dt*k1_I, R + 0.5*dt*k1_R, D + 0.5*dt*k1_D, day, 0.5)\n",
    "        k3_S, k3_I, k3_R, k3_D = sird_derivs(S + 0.5*dt*k2_S, I + 0.5*dt*k2_I, R + 0.5*dt*k2_R, D + 0.5*dt*k2_D, day, 0.5)\n",
    "        k4_S, k4_I, k4_R, k4_D = sird_derivs(S + dt*k3_S, I + dt*k3_I, R + dt*k3_R, D + dt*k3_D, day, 1.0)\n",
    "        S_new = S + (dt/6)*(k1_S + 2*k2_S + 2*k3_S + k4_S)\n",
    "        I_new = I + (dt/6)*(k1_I + 2*k2_I + 2*k3_I + k4_I)\n",
    "        R_new = R + (dt/6)*(k1_R + 2*k2_R + 2*k3_R + k4_R)\n",
    "        D_new = D + (dt/6)*(k1_D + 2*k2_D + 2*k3_D + k4_D)\n",
    "        S_new = max(S_new, 0)\n",
    "        I_new = max(I_new, 0)\n",
    "        R_new = max(R_new, 0)\n",
    "        D_new = max(D_new, 0)\n",
    "        total = S_new + I_new + R_new + D_new\n",
    "        if abs(total - Npop) > 1e-6:\n",
    "            scale = Npop / (total + 1e-9)\n",
    "            S_new *= scale\n",
    "            I_new *= scale\n",
    "            R_new *= scale\n",
    "            D_new *= scale\n",
    "        S, I, R, D = S_new, I_new, R_new, D_new\n",
    "    return S_arr, I_arr, R_arr, D_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9a008",
   "metadata": {},
   "source": [
    "### 3.2 Optymalizacja  PSO z akceleracją GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba714b",
   "metadata": {},
   "source": [
    "#### 3.2.1 Kernel GPU dla obliczeń SIRD i kosztu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d06a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def sird_rk4_gpu(beta1_arr, beta2_arr, t1_arr, t2_arr, gamma_arr, mu_arr, cost_arr, dt, Npop, days, I_emp, R_emp, D_emp, S0, I0, R0, D0, use_norm, i_min, i_rng, r_min, r_rng, d_min, d_rng, d_weight=5.0, i_weight=2.0):\n",
    "    pid = cuda.grid(1)\n",
    "    if pid < beta1_arr.size:\n",
    "        beta1 = beta1_arr[pid]\n",
    "        beta2 = beta2_arr[pid]\n",
    "        t1 = t1_arr[pid]\n",
    "        t2 = t2_arr[pid]\n",
    "        gamma_ = gamma_arr[pid]\n",
    "        mu_ = mu_arr[pid]\n",
    "        S = S0\n",
    "        I = I0\n",
    "        R = R0\n",
    "        D = D0\n",
    "        MAX_DAYS_GPU = 1500\n",
    "        if days > MAX_DAYS_GPU:\n",
    "            cost_arr[pid] = 1e30\n",
    "            return\n",
    "        I_sim = cuda.local.array(MAX_DAYS_GPU, dtype=numba.float32)\n",
    "        R_sim = cuda.local.array(MAX_DAYS_GPU, dtype=numba.float32)\n",
    "        D_sim = cuda.local.array(MAX_DAYS_GPU, dtype=numba.float32)\n",
    "\n",
    "        def sird_derivs_kernel(S_loc, I_loc, day_idx, substep_frac):\n",
    "            curr = day_idx + substep_frac\n",
    "            beta_t = beta1 # Domyślnie\n",
    "            if curr < t1:\n",
    "                beta_t = beta1\n",
    "            elif curr < t2:\n",
    "                beta_t = beta1 + (curr - t1) / (t2 - t1 + 1e-8) * (beta2 - beta1)\n",
    "            else:\n",
    "                beta_t = beta2\n",
    "            dS = -beta_t * S_loc * I_loc / Npop\n",
    "            dI = beta_t * S_loc * I_loc / Npop - (gamma_ + mu_) * I_loc\n",
    "            dR = gamma_ * I_loc\n",
    "            dD = mu_ * I_loc\n",
    "            return dS, dI, dR, dD\n",
    "\n",
    "        for day in range(days):\n",
    "            k1S, k1I, k1R, k1D = sird_derivs_kernel(S, I, day, 0.0)\n",
    "            k2S, k2I, k2R, k2D = sird_derivs_kernel(S + 0.5*dt*k1S, I + 0.5*dt*k1I, day, 0.5)\n",
    "            k3S, k3I, k3R, k3D = sird_derivs_kernel(S + 0.5*dt*k2S, I + 0.5*dt*k2I, day, 0.5)\n",
    "            k4S, k4I, k4R, k4D = sird_derivs_kernel(S + dt*k3S, I + dt*k3I, day, 1.0)\n",
    "            Sn = S + (dt/6)*(k1S+2*k2S+2*k3S+k4S)\n",
    "            In = I + (dt/6)*(k1I+2*k2I+2*k3I+k4I)\n",
    "            Rn = R + (dt/6)*(k1R+2*k2R+2*k3R+k4R)\n",
    "            Dn = D + (dt/6)*(k1D+2*k2D+2*k3D+k4D)\n",
    "            Sn = max(Sn, 0)\n",
    "            In = max(In, 0)\n",
    "            Rn = max(Rn, 0)\n",
    "            Dn = max(Dn, 0)\n",
    "            tot = Sn + In + Rn + Dn\n",
    "            if abs(tot - Npop) > 1e-6:\n",
    "                scale = Npop / (tot + 1e-9)\n",
    "                Sn *= scale\n",
    "                In *= scale\n",
    "                Rn *= scale\n",
    "                Dn *= scale\n",
    "            I_sim[day] = In\n",
    "            R_sim[day] = Rn\n",
    "            D_sim[day] = Dn\n",
    "            S = Sn\n",
    "            I = In\n",
    "            R = Rn\n",
    "            D = Dn\n",
    "\n",
    "        i_err = 0.0\n",
    "        r_err = 0.0\n",
    "        d_err = 0.0\n",
    "        for day in range(days):\n",
    "            di = 0.0\n",
    "            dr = 0.0\n",
    "            dd = 0.0\n",
    "            if use_norm == 1:\n",
    "                if i_rng > 1e-12: di = ((I_sim[day] - i_min)/i_rng) - ((I_emp[day] - i_min)/i_rng)\n",
    "                if r_rng > 1e-12: dr = ((R_sim[day] - r_min)/r_rng) - ((R_emp[day] - r_min)/r_rng)\n",
    "                if d_rng > 1e-12: dd = ((D_sim[day] - d_min)/d_rng) - ((D_emp[day] - d_min)/d_rng)\n",
    "            else:\n",
    "                di = I_sim[day] - I_emp[day]\n",
    "                dr = R_sim[day] - R_emp[day]\n",
    "                dd = D_sim[day] - D_emp[day]\n",
    "            i_err += abs(di)\n",
    "            r_err += abs(dr)\n",
    "            d_err += abs(dd)\n",
    "\n",
    "        tot_err = i_err + r_err + d_err\n",
    "        ad_i_w = i_weight\n",
    "        ad_r_w = 1.0\n",
    "        ad_d_w = d_weight\n",
    "        if tot_err > 1e-10:\n",
    "            ad_i_w = i_weight * (1.0 + i_err / tot_err)\n",
    "            ad_r_w = 1.0 * (1.0 + r_err / tot_err)\n",
    "            ad_d_w = d_weight * (1.0 + d_err / tot_err)\n",
    "\n",
    "        sse = 0.0\n",
    "        for day in range(days):\n",
    "            di = 0.0\n",
    "            dr = 0.0\n",
    "            dd = 0.0\n",
    "            if use_norm == 1:\n",
    "                if i_rng > 1e-12: di = ((I_sim[day] - i_min)/i_rng) - ((I_emp[day] - i_min)/i_rng)\n",
    "                if r_rng > 1e-12: dr = ((R_sim[day] - r_min)/r_rng) - ((R_emp[day] - r_min)/r_rng)\n",
    "                if d_rng > 1e-12: dd = ((D_sim[day] - d_min)/d_rng) - ((D_emp[day] - d_min)/d_rng)\n",
    "            else:\n",
    "                di = I_sim[day] - I_emp[day]\n",
    "                dr = R_sim[day] - R_emp[day]\n",
    "                dd = D_sim[day] - D_emp[day]\n",
    "            sse += di*di*ad_i_w + dr*dr*ad_r_w + dd*dd*ad_d_w\n",
    "        cost_arr[pid] = sse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712795c0",
   "metadata": {},
   "source": [
    "#### 3.2.2 Funkcje pomocnicze PSO (LHS, Inicjalizacja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0644e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latin_hypercube_sampling(n_samples, bounds, correlation_reduction=True):\n",
    "    params = list(bounds.keys())\n",
    "    n_params = len(params)\n",
    "    lower_bounds = [bounds[p][0] for p in params]\n",
    "    upper_bounds = [bounds[p][1] for p in params]\n",
    "    if correlation_reduction:\n",
    "        sampler = qmc.LatinHypercube(d=n_params, optimization=\"random-cd\", seed=42)\n",
    "        samples = sampler.random(n=n_samples)\n",
    "        for _ in range(5):\n",
    "            candidate = qmc.LatinHypercube(d=n_params, optimization=\"random-cd\", seed=np.random.randint(1000)).random(n=n_samples)\n",
    "            if qmc.discrepancy(candidate) < qmc.discrepancy(samples):\n",
    "                samples = candidate\n",
    "    else:\n",
    "        sampler = qmc.LatinHypercube(d=n_params, seed=42)\n",
    "        samples = sampler.random(n=n_samples)\n",
    "    scaled_samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
    "    result = {param: scaled_samples[:, i] for i, param in enumerate(params)}\n",
    "    return result\n",
    "\n",
    "def initialize_pso_with_lhs(n_particles, bounds):\n",
    "    return latin_hypercube_sampling(n_particles, bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b338618",
   "metadata": {},
   "source": [
    "#### 3.2.3 Główna funkcja PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281086ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_pso_sird_gpu(\n",
    "    days, D_emp, I_emp=None, R_emp=None, S0=0.0, I0=0.0, R0=0.0, D0=0.0,\n",
    "    dt=DT, Npop=POPULATION, n_particles=NUM_PARTICLES, max_iter=MAX_ITER,\n",
    "    bounds_beta1=(0.01, 1.5), bounds_beta2=(0.01, 1.5), bounds_t1=(0.0, 15.0),\n",
    "    bounds_t2=(10.0, 40.0), bounds_gamma=(0.01, 0.3), bounds_mu=(0.001, 0.05),\n",
    "    use_norm=True, d_weight=5.0, i_weight=2.0, r_weight=1.0,\n",
    "    use_lhs=True, adaptive_inertia=True, use_islands=True, num_islands=4\n",
    "):\n",
    "    print(\"Uruchamianie ulepszonego PSO-SIRD z:\")\n",
    "    print(f\"- LHS: {'Tak' if use_lhs else 'Nie'}, Adaptacyjna bezwładność: {'Tak' if adaptive_inertia else 'Nie'}, Wyspy: {'Tak' if use_islands else 'Nie'}, Cząsteczki: {n_particles}\")\n",
    "    W_init, W_final = (0.9, 0.4) if adaptive_inertia else (0.5, 0.5)\n",
    "    n_islands, island_size = (0, 0) \n",
    "    if use_islands:\n",
    "        safe_num_islands = min(num_islands, n_particles // 100) if n_particles >= 100 else 1 \n",
    "        n_islands = max(1, safe_num_islands) \n",
    "        island_size = n_particles // n_islands\n",
    "        print(f\"- Liczba wysp: {n_islands} (po {island_size} cząstek)\")\n",
    "\n",
    "    C1_pso, C2_pso = 1.5, 1.5\n",
    "    param_bounds = {'beta1': bounds_beta1, 'beta2': bounds_beta2, 't1': bounds_t1, 't2': bounds_t2, 'gamma': bounds_gamma, 'mu': bounds_mu}\n",
    "    I_emp = np.zeros(days, dtype=np.float32) if I_emp is None else I_emp.astype(np.float32)\n",
    "    R_emp = np.zeros(days, dtype=np.float32) if R_emp is None else R_emp.astype(np.float32)\n",
    "    D_emp = D_emp.astype(np.float32)\n",
    "\n",
    "    if use_lhs:\n",
    "        try:\n",
    "            samples = initialize_pso_with_lhs(n_particles, param_bounds)\n",
    "            beta1, beta2, t1, t2, gamma, mu = samples['beta1'], samples['beta2'], samples['t1'], samples['t2'], samples['gamma'], samples['mu']\n",
    "            t2 = np.maximum(t2, t1 + 1.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd LHS: {e}. Używam standardowej inicjalizacji.\")\n",
    "            use_lhs = False\n",
    "    if not use_lhs:\n",
    "        beta1 = np.random.uniform(bounds_beta1[0], bounds_beta1[1], n_particles)\n",
    "        beta2 = np.random.uniform(bounds_beta2[0], bounds_beta2[1], n_particles)\n",
    "        t1 = np.random.uniform(bounds_t1[0], bounds_t1[1], n_particles)\n",
    "        t2 = np.random.uniform(bounds_t2[0], bounds_t2[1], n_particles)\n",
    "        gamma = np.random.uniform(bounds_gamma[0], bounds_gamma[1], n_particles)\n",
    "        mu = np.random.uniform(bounds_mu[0], bounds_mu[1], n_particles)\n",
    "        t2 = np.maximum(t2, t1 + 5.0)\n",
    "\n",
    "    v_beta1, v_beta2, v_t1, v_t2, v_gamma, v_mu = (np.zeros(n_particles, dtype=np.float32) for _ in range(6))\n",
    "    pbest_beta1, pbest_beta2, pbest_t1, pbest_t2, pbest_gamma, pbest_mu = beta1.copy(), beta2.copy(), t1.copy(), t2.copy(), gamma.copy(), mu.copy()\n",
    "    pbest_cost = np.full(n_particles, 1e30, dtype=np.float32)\n",
    "    gbest_cost = 1e30\n",
    "    gbest_params = {\"beta1\": 0.0, \"beta2\": 0.0, \"t1\": 0.0, \"t2\": 0.0, \"gamma\": 0.0, \"mu\": 0.0}\n",
    "\n",
    "    if use_norm:\n",
    "        i_min, i_max = np.min(I_emp), np.max(I_emp)\n",
    "        r_min, r_max = np.min(R_emp), np.max(R_emp)\n",
    "        d_min, d_max = np.min(D_emp), np.max(D_emp)\n",
    "        i_rng = max(i_max - i_min, 1e-6)\n",
    "        r_rng = max(r_max - r_min, 1e-6)\n",
    "        d_rng = max(d_max - d_min, 1e-6)\n",
    "    else:\n",
    "        i_min, i_rng, r_min, r_rng, d_min, d_rng = 0.0, 1.0, 0.0, 1.0, 0.0, 1.0\n",
    "\n",
    "    D_emp_gpu = cuda.to_device(D_emp)\n",
    "    I_emp_gpu = cuda.to_device(I_emp)\n",
    "    R_emp_gpu = cuda.to_device(R_emp)\n",
    "    beta1_gpu = cuda.to_device(beta1.astype(np.float32))\n",
    "    beta2_gpu = cuda.to_device(beta2.astype(np.float32))\n",
    "    t1_gpu = cuda.to_device(t1.astype(np.float32))\n",
    "    t2_gpu = cuda.to_device(t2.astype(np.float32))\n",
    "    gamma_gpu = cuda.to_device(gamma.astype(np.float32))\n",
    "    mu_gpu = cuda.to_device(mu.astype(np.float32))\n",
    "    cost_gpu = cuda.device_array(n_particles, dtype=np.float32)\n",
    "    norm_data = np.array([i_min, i_rng, r_min, r_rng, d_min, d_rng], dtype=np.float32)\n",
    "    norm_data_gpu = cuda.to_device(norm_data)\n",
    "    use_norm_flag = 1 if use_norm else 0\n",
    "    threadsperblock = 256\n",
    "    blockspergrid = (n_particles + threadsperblock - 1) // threadsperblock\n",
    "\n",
    "    if use_islands:\n",
    "        island_indices = [np.arange(i * island_size, (i + 1) * island_size if i < n_islands - 1 else n_particles) for i in range(n_islands)]\n",
    "        island_best_cost = np.full(n_islands, 1e30, dtype=np.float32)\n",
    "        island_best_params = [{\"beta1\": 0.0, \"beta2\": 0.0, \"t1\": 0.0, \"t2\": 0.0, \"gamma\": 0.0, \"mu\": 0.0} for _ in range(n_islands)]\n",
    "\n",
    "    history = []\n",
    "    no_improvement_count = 0\n",
    "    start_time = time.time()\n",
    "    for iteration in range(max_iter):\n",
    "        if iteration % 20 == 0 or iteration == max_iter - 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Iteracja {iteration}/{max_iter}: Najlepszy koszt = {gbest_cost:.8f}, czas: {elapsed:.2f}s\")\n",
    "        w = W_init - (W_init - W_final) * iteration / max_iter if adaptive_inertia else W_init\n",
    "        sird_rk4_gpu[blockspergrid, threadsperblock](\n",
    "            beta1_gpu, beta2_gpu, t1_gpu, t2_gpu, gamma_gpu, mu_gpu, cost_gpu,\n",
    "            dt, Npop, days, I_emp_gpu, R_emp_gpu, D_emp_gpu,\n",
    "            S0, I0, R0, D0, use_norm_flag,\n",
    "            norm_data_gpu[0], norm_data_gpu[1], norm_data_gpu[2],\n",
    "            norm_data_gpu[3], norm_data_gpu[4], norm_data_gpu[5],\n",
    "            d_weight, i_weight\n",
    "        )\n",
    "        cuda.synchronize()\n",
    "        cost_values = cost_gpu.copy_to_host()\n",
    "        better = cost_values < pbest_cost\n",
    "        pbest_cost[better] = cost_values[better]\n",
    "        pbest_beta1[better] = beta1[better]\n",
    "        pbest_beta2[better] = beta2[better]\n",
    "        pbest_t1[better] = t1[better]\n",
    "        pbest_t2[better] = t2[better]\n",
    "        pbest_gamma[better] = gamma[better]\n",
    "        pbest_mu[better] = mu[better]\n",
    "        min_idx = np.argmin(cost_values)\n",
    "        min_cost = cost_values[min_idx]\n",
    "        if min_cost < gbest_cost:\n",
    "            gbest_cost = min_cost\n",
    "            gbest_params = {\"beta1\": beta1[min_idx], \"beta2\": beta2[min_idx], \"t1\": t1[min_idx],\n",
    "                            \"t2\": t2[min_idx], \"gamma\": gamma[min_idx], \"mu\": mu[min_idx]}\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "        history.append(gbest_cost)\n",
    "        if use_islands:\n",
    "            for i, indices in enumerate(island_indices):\n",
    "                island_min_idx = indices[np.argmin(cost_values[indices])]\n",
    "                island_min_cost = cost_values[island_min_idx]\n",
    "                if island_min_cost < island_best_cost[i]:\n",
    "                    island_best_cost[i] = island_min_cost\n",
    "                    island_best_params[i] = {\"beta1\": beta1[island_min_idx], \"beta2\": beta2[island_min_idx],\n",
    "                                             \"t1\": t1[island_min_idx], \"t2\": t2[island_min_idx],\n",
    "                                             \"gamma\": gamma[island_min_idx], \"mu\": mu[island_min_idx]}\n",
    "        if no_improvement_count > 30:\n",
    "            print(f\"Wczesne zatrzymanie po {iteration} iteracjach\")\n",
    "            break\n",
    "\n",
    "        if use_islands:\n",
    "            for i, indices in enumerate(island_indices):\n",
    "                r1, r2, r3 = np.random.rand(len(indices)), np.random.rand(len(indices)), np.random.rand(len(indices))\n",
    "                gw, lw = 0.7, 0.3\n",
    "                v_beta1[indices] = w * v_beta1[indices] + C1_pso * r1 * (pbest_beta1[indices] - beta1[indices]) + C2_pso * gw * r2 * (gbest_params[\"beta1\"] - beta1[indices]) + C2_pso * lw * r3 * (island_best_params[i][\"beta1\"] - beta1[indices])\n",
    "                r1, r2, r3 = np.random.rand(len(indices)), np.random.rand(len(indices)), np.random.rand(len(indices))\n",
    "                v_beta2[indices] = w * v_beta2[indices] + C1_pso * r1 * (pbest_beta2[indices] - beta2[indices]) + C2_pso * gw * r2 * (gbest_params[\"beta2\"] - beta2[indices]) + C2_pso * lw * r3 * (island_best_params[i][\"beta2\"] - beta2[indices])\n",
    "                r1, r2, r3 = np.random.rand(len(indices)), np.random.rand(len(indices)), np.random.rand(len(indices))\n",
    "                v_t1[indices] = w * v_t1[indices] + C1_pso * r1 * (pbest_t1[indices] - t1[indices]) + C2_pso * gw * r2 * (gbest_params[\"t1\"] - t1[indices]) + C2_pso * lw * r3 * (island_best_params[i][\"t1\"] - t1[indices])\n",
    "                r1, r2, r3 = np.random.rand(len(indices)), np.random.rand(len(indices)), np.random.rand(len(indices))\n",
    "                v_t2[indices] = w * v_t2[indices] + C1_pso * r1 * (pbest_t2[indices] - t2[indices]) + C2_pso * gw * r2 * (gbest_params[\"t2\"] - t2[indices]) + C2_pso * lw * r3 * (island_best_params[i][\"t2\"] - t2[indices])\n",
    "                r1, r2, r3 = np.random.rand(len(indices)), np.random.rand(len(indices)), np.random.rand(len(indices))\n",
    "                v_gamma[indices] = w * v_gamma[indices] + C1_pso * r1 * (pbest_gamma[indices] - gamma[indices]) + C2_pso * gw * r2 * (gbest_params[\"gamma\"] - gamma[indices]) + C2_pso * lw * r3 * (island_best_params[i][\"gamma\"] - gamma[indices])\n",
    "                r1, r2, r3 = np.random.rand(len(indices)), np.random.rand(len(indices)), np.random.rand(len(indices))\n",
    "                v_mu[indices] = w * v_mu[indices] + C1_pso * r1 * (pbest_mu[indices] - mu[indices]) + C2_pso * gw * r2 * (gbest_params[\"mu\"] - mu[indices]) + C2_pso * lw * r3 * (island_best_params[i][\"mu\"] - mu[indices])\n",
    "        else:\n",
    "            r1, r2 = np.random.rand(n_particles), np.random.rand(n_particles)\n",
    "            v_beta1 = w * v_beta1 + C1_pso * r1 * (pbest_beta1 - beta1) + C2_pso * r2 * (gbest_params[\"beta1\"] - beta1)\n",
    "            r1, r2 = np.random.rand(n_particles), np.random.rand(n_particles)\n",
    "            v_beta2 = w * v_beta2 + C1_pso * r1 * (pbest_beta2 - beta2) + C2_pso * r2 * (gbest_params[\"beta2\"] - beta2)\n",
    "            r1, r2 = np.random.rand(n_particles), np.random.rand(n_particles)\n",
    "            v_t1 = w * v_t1 + C1_pso * r1 * (pbest_t1 - t1) + C2_pso * r2 * (gbest_params[\"t1\"] - t1)\n",
    "            r1, r2 = np.random.rand(n_particles), np.random.rand(n_particles)\n",
    "            v_t2 = w * v_t2 + C1_pso * r1 * (pbest_t2 - t2) + C2_pso * r2 * (gbest_params[\"t2\"] - t2)\n",
    "            r1, r2 = np.random.rand(n_particles), np.random.rand(n_particles)\n",
    "            v_gamma = w * v_gamma + C1_pso * r1 * (pbest_gamma - gamma) + C2_pso * r2 * (gbest_params[\"gamma\"] - gamma)\n",
    "            r1, r2 = np.random.rand(n_particles), np.random.rand(n_particles)\n",
    "            v_mu = w * v_mu + C1_pso * r1 * (pbest_mu - mu) + C2_pso * r2 * (gbest_params[\"mu\"] - mu)\n",
    "\n",
    "        beta1 += v_beta1\n",
    "        beta2 += v_beta2\n",
    "        t1 += v_t1\n",
    "        t2 += v_t2\n",
    "        gamma += v_gamma\n",
    "        mu += v_mu\n",
    "\n",
    "        beta1 = np.clip(beta1, bounds_beta1[0], bounds_beta1[1])\n",
    "        beta2 = np.clip(beta2, bounds_beta2[0], bounds_beta2[1])\n",
    "        t1 = np.clip(t1, bounds_t1[0], bounds_t1[1])\n",
    "        t2 = np.clip(t2, bounds_t2[0], bounds_t2[1])\n",
    "        t2 = np.maximum(t2, t1 + 1.0)\n",
    "        gamma = np.clip(gamma, bounds_gamma[0], bounds_gamma[1])\n",
    "        mu = np.clip(mu, bounds_mu[0], bounds_mu[1])\n",
    "\n",
    "        beta1_gpu.copy_to_device(beta1.astype(np.float32))\n",
    "        beta2_gpu.copy_to_device(beta2.astype(np.float32))\n",
    "        t1_gpu.copy_to_device(t1.astype(np.float32))\n",
    "        t2_gpu.copy_to_device(t2.astype(np.float32))\n",
    "        gamma_gpu.copy_to_device(gamma.astype(np.float32))\n",
    "        mu_gpu.copy_to_device(mu.astype(np.float32))\n",
    "\n",
    "        if no_improvement_count > 15 and iteration < max_iter * 0.7:\n",
    "            print(f\"Zastosowanie mechanizmu różnorodności w iteracji {iteration}\")\n",
    "            n_reinit = int(n_particles * 0.25)\n",
    "            reinit_indices = np.random.choice(n_particles, n_reinit, replace=False)\n",
    "            loc_idx = reinit_indices[:n_reinit//3]\n",
    "            glob_idx = reinit_indices[n_reinit//3:2*n_reinit//3]\n",
    "            cross_idx = reinit_indices[2*n_reinit//3:]\n",
    "            if len(loc_idx) > 0:\n",
    "                beta1[loc_idx] = np.random.normal(gbest_params[\"beta1\"], abs(gbest_params[\"beta1\"]) * 0.2, len(loc_idx))\n",
    "                beta2[loc_idx] = np.random.normal(gbest_params[\"beta2\"], abs(gbest_params[\"beta2\"]) * 0.2, len(loc_idx))\n",
    "                t1[loc_idx] = np.random.normal(gbest_params[\"t1\"], max(1.0, abs(gbest_params[\"t1\"]) * 0.2), len(loc_idx))\n",
    "                t2[loc_idx] = np.random.normal(gbest_params[\"t2\"], max(1.0, abs(gbest_params[\"t2\"]) * 0.2), len(loc_idx))\n",
    "                gamma[loc_idx] = np.random.normal(gbest_params[\"gamma\"], abs(gbest_params[\"gamma\"]) * 0.2, len(loc_idx))\n",
    "                mu[loc_idx] = np.random.normal(gbest_params[\"mu\"], abs(gbest_params[\"mu\"]) * 0.2, len(loc_idx))\n",
    "            if len(glob_idx) > 0:\n",
    "                beta1[glob_idx] = np.random.uniform(bounds_beta1[0], bounds_beta1[1], len(glob_idx))\n",
    "                beta2[glob_idx] = np.random.uniform(bounds_beta2[0], bounds_beta2[1], len(glob_idx))\n",
    "                t1[glob_idx] = np.random.uniform(bounds_t1[0], bounds_t1[1], len(glob_idx))\n",
    "                t2[glob_idx] = np.random.uniform(bounds_t2[0], bounds_t2[1], len(glob_idx))\n",
    "                gamma[glob_idx] = np.random.uniform(bounds_gamma[0], bounds_gamma[1], len(glob_idx))\n",
    "                mu[glob_idx] = np.random.uniform(bounds_mu[0], bounds_mu[1], len(glob_idx))\n",
    "            if len(cross_idx) > 0:\n",
    "                best_indices = np.argsort(pbest_cost)[:20]\n",
    "                for idx in cross_idx:\n",
    "                    p1, p2 = np.random.choice(best_indices, 2, replace=False)\n",
    "                    alpha = np.random.uniform(0.3, 0.7)\n",
    "                    mut = 0.1\n",
    "                    beta1[idx] = (beta1[p1] * alpha + beta1[p2] * (1 - alpha)) * np.random.uniform(1-mut, 1+mut)\n",
    "                    beta2[idx] = (beta2[p1] * alpha + beta2[p2] * (1 - alpha)) * np.random.uniform(1-mut, 1+mut)\n",
    "                    t1[idx] = (t1[p1] * alpha + t1[p2] * (1 - alpha)) * np.random.uniform(1-mut, 1+mut)\n",
    "                    t2[idx] = (t2[p1] * alpha + t2[p2] * (1 - alpha)) * np.random.uniform(1-mut, 1+mut)\n",
    "                    gamma[idx] = (gamma[p1] * alpha + gamma[p2] * (1 - alpha)) * np.random.uniform(1-mut, 1+mut)\n",
    "                    mu[idx] = (mu[p1] * alpha + mu[p2] * (1 - alpha)) * np.random.uniform(1-mut, 1+mut)\n",
    "            beta1 = np.clip(beta1, bounds_beta1[0], bounds_beta1[1])\n",
    "            beta2 = np.clip(beta2, bounds_beta2[0], bounds_beta2[1])\n",
    "            t1 = np.clip(t1, bounds_t1[0], bounds_t1[1])\n",
    "            t2 = np.clip(t2, bounds_t2[0], bounds_t2[1])\n",
    "            t2 = np.maximum(t2, t1 + 1.0)\n",
    "            gamma = np.clip(gamma, bounds_gamma[0], bounds_gamma[1])\n",
    "            mu = np.clip(mu, bounds_mu[0], bounds_mu[1])\n",
    "            v_beta1[reinit_indices] = 0\n",
    "            v_beta2[reinit_indices] = 0\n",
    "            v_t1[reinit_indices] = 0\n",
    "            v_t2[reinit_indices] = 0\n",
    "            v_gamma[reinit_indices] = 0\n",
    "            v_mu[reinit_indices] = 0\n",
    "            beta1_gpu.copy_to_device(beta1.astype(np.float32))\n",
    "            beta2_gpu.copy_to_device(beta2.astype(np.float32))\n",
    "            t1_gpu.copy_to_device(t1.astype(np.float32))\n",
    "            t2_gpu.copy_to_device(t2.astype(np.float32))\n",
    "            gamma_gpu.copy_to_device(gamma.astype(np.float32))\n",
    "            mu_gpu.copy_to_device(mu.astype(np.float32))\n",
    "\n",
    "    best_params_final = {k: float(v) for k, v in gbest_params.items()}\n",
    "    del D_emp_gpu, I_emp_gpu, R_emp_gpu, beta1_gpu, beta2_gpu, t1_gpu, t2_gpu, gamma_gpu, mu_gpu, cost_gpu, norm_data_gpu\n",
    "    print(f\"PSO zakończone po {len(history)} iteracjach w {time.time() - start_time:.2f} s. Najlepszy koszt: {gbest_cost:.8f}. Parametry: {best_params_final}\")\n",
    "    return best_params_final, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8ae9f",
   "metadata": {},
   "source": [
    "#### 3.2.4 Wielofazowe PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_phase_pso(df, start_date, end_date, forecast_days=21, use_lhs=True,\n",
    "                   adaptive_inertia=True, use_islands=True, population=POPULATION,\n",
    "                   dt=DT, i_weight=2.0, d_weight=5.0, use_norm=True):\n",
    "    dfw = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    dfw.reset_index(drop=True, inplace=True)\n",
    "    days_window = len(dfw)\n",
    "\n",
    "    print(f\"Wielofazowe PSO: Dopasowanie dla okresu {start_date.date()} do {end_date.date()} ({days_window} dni)\")\n",
    "\n",
    "    I_emp = dfw[\"Active\"].values.astype(np.float32)\n",
    "    R_emp = dfw[\"Recovered\"].values.astype(np.float32)\n",
    "    D_emp = dfw[\"Deaths\"].values.astype(np.float32)\n",
    "\n",
    "    row0 = dfw.iloc[0]\n",
    "    S0 = population - (row0[\"Active\"] + row0[\"Recovered\"] + row0[\"Deaths\"])\n",
    "    I0 = row0[\"Active\"]\n",
    "    R0 = row0[\"Recovered\"]\n",
    "    D0 = row0[\"Deaths\"]\n",
    "\n",
    "    print(\"Faza 1: Szeroka eksploracja przestrzeni parametrów\")\n",
    "    wide_bounds_beta1 = (0.01, 1.5)\n",
    "    wide_bounds_beta2 = (0.01, 1.5)\n",
    "    wide_bounds_t1 = (0.0, 15.0)\n",
    "    wide_bounds_t2 = (10.0, 40.0)\n",
    "    wide_bounds_gamma = (0.01, 0.3)\n",
    "    wide_bounds_mu = (0.001, 0.05)\n",
    "\n",
    "    phase1_params, phase1_history = improved_pso_sird_gpu(\n",
    "        days=days_window, D_emp=D_emp, I_emp=I_emp, R_emp=R_emp,\n",
    "        S0=S0, I0=I0, R0=R0, D0=D0, dt=dt, Npop=population,\n",
    "        n_particles=NUM_PARTICLES, max_iter=MAX_ITER,\n",
    "        bounds_beta1=wide_bounds_beta1, bounds_beta2=wide_bounds_beta2,\n",
    "        bounds_t1=wide_bounds_t1, bounds_t2=wide_bounds_t2,\n",
    "        bounds_gamma=wide_bounds_gamma, bounds_mu=wide_bounds_mu,\n",
    "        use_norm=use_norm, d_weight=d_weight, i_weight=i_weight,\n",
    "        use_lhs=use_lhs, adaptive_inertia=adaptive_inertia, use_islands=use_islands\n",
    "    )\n",
    "\n",
    "    print(\"Faza 2: Dokładne dopasowanie w węższym obszarze\")\n",
    "    narrow_bounds_beta1 = (max(0.01, phase1_params[\"beta1\"] * 0.7), min(1.5, phase1_params[\"beta1\"] * 1.3))\n",
    "    narrow_bounds_beta2 = (max(0.01, phase1_params[\"beta2\"] * 0.7), min(1.5, phase1_params[\"beta2\"] * 1.3))\n",
    "    narrow_bounds_t1 = (max(0.0, phase1_params[\"t1\"] - 2.5), min(15.0, phase1_params[\"t1\"] + 2.5))\n",
    "    narrow_bounds_t2 = (max(10.0, phase1_params[\"t2\"] - 4.0), min(40.0, phase1_params[\"t2\"] + 4.0))\n",
    "    narrow_bounds_gamma = (max(0.01, phase1_params[\"gamma\"] * 0.7), min(0.3, phase1_params[\"gamma\"] * 1.3))\n",
    "    narrow_bounds_mu = (max(0.001, phase1_params[\"mu\"] * 0.7), min(0.05, phase1_params[\"mu\"] * 1.3))\n",
    "\n",
    "    if narrow_bounds_t2[0] <= narrow_bounds_t1[1]:\n",
    "        narrow_bounds_t2 = (narrow_bounds_t1[1] + 1.0, narrow_bounds_t2[1])\n",
    "\n",
    "    phase2_params, phase2_history = improved_pso_sird_gpu(\n",
    "        days=days_window, D_emp=D_emp, I_emp=I_emp, R_emp=R_emp,\n",
    "        S0=S0, I0=I0, R0=R0, D0=D0, dt=DT, Npop=population,\n",
    "        n_particles=NUM_PARTICLES, max_iter=MAX_ITER,\n",
    "        bounds_beta1=narrow_bounds_beta1, bounds_beta2=narrow_bounds_beta2,\n",
    "        bounds_t1=narrow_bounds_t1, bounds_t2=narrow_bounds_t2,\n",
    "        bounds_gamma=narrow_bounds_gamma, bounds_mu=narrow_bounds_mu,\n",
    "        use_norm=True, d_weight=5.0, i_weight=2.0,\n",
    "        use_lhs=False, adaptive_inertia=True, use_islands=True\n",
    "    )\n",
    "\n",
    "    phase1_cost = phase1_history[-1] if phase1_history else float('inf')\n",
    "    phase2_cost = phase2_history[-1] if phase2_history else float('inf')\n",
    "\n",
    "    if phase2_cost < phase1_cost:\n",
    "        print(\"Wybrano parametry z Fazy 2 (dokładne dopasowanie).\")\n",
    "        return phase2_params, phase2_history\n",
    "    else:\n",
    "        print(\"Wybrano parametry z Fazy 1 (szeroka eksploracja).\")\n",
    "        return phase1_params, phase1_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de11845",
   "metadata": {},
   "source": [
    "### 3.3 Modele Korekcji Residuów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc8ad0",
   "metadata": {},
   "source": [
    "### 3.3.1 Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerResidualModel:\n",
    "    \"\"\"Model transformatora. (Poprawiono formatowanie)\"\"\"\n",
    "    def __init__(self, seq_length=14, forecast_horizon=14, d_model=64, num_heads=4,\n",
    "                 num_transformer_blocks=2, mlp_units=[128, 64], dropout=0.1,\n",
    "                 learning_rate=0.001, batch_size=32, random_seed=42):\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.mlp_units = mlp_units\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.random_seed = random_seed\n",
    "        self.input_dim = 3\n",
    "        self.scalers = {'I': StandardScaler(), 'R': StandardScaler(), 'D': StandardScaler()}\n",
    "        self.model = self._build_model()\n",
    "        self.is_trained = False\n",
    "\n",
    "    def _analyze_noise_level(self, residuals_dict):\n",
    "        noise_levels = {}\n",
    "        for comp in ['I', 'R', 'D']:\n",
    "            values = residuals_dict[comp]\n",
    "            noise_levels[comp] = np.std(values) / (np.mean(np.abs(values)) + 1e-6) if len(values) > 10 else 0.5\n",
    "        mean_noise = np.mean(list(noise_levels.values()))\n",
    "        self.dropout = min(0.3, self.dropout * 1.5) if mean_noise > 0.5 else max(0.05, self.dropout * 0.8)\n",
    "        print(f\"Poziomy szumu: {noise_levels}, dostosowany dropout: {self.dropout}\")\n",
    "        return noise_levels\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.seq_length, self.input_dim))\n",
    "        x = layers.Conv1D(filters=self.d_model, kernel_size=1, activation='relu')(inputs)\n",
    "        x = self._positional_encoding(x)\n",
    "        for _ in range(self.num_transformer_blocks):\n",
    "            x = self._transformer_block(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        for dim in self.mlp_units:\n",
    "            x = layers.Dense(dim, activation='relu')(x)\n",
    "            x = layers.Dropout(self.dropout)(x)\n",
    "        outputs = layers.Dense(self.forecast_horizon * 3)(x)\n",
    "        outputs = layers.Reshape((self.forecast_horizon, 3))(outputs)\n",
    "        model = models.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss=MeanSquaredError())\n",
    "        return model\n",
    "\n",
    "    def _positional_encoding(self, x):\n",
    "        positions = np.arange(self.seq_length)[:, np.newaxis]\n",
    "        indices = np.arange(self.d_model)[np.newaxis, :]\n",
    "        angles = positions * (1 / np.power(10000, (2 * (indices // 2)) / self.d_model))\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return x + tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def _transformer_block(self, x):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.d_model // self.num_heads)(x, x)\n",
    "        attn_output = layers.Dropout(self.dropout)(attn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn = layers.Dense(self.d_model * 4, activation='relu')(x)\n",
    "        ffn = layers.Dense(self.d_model)(ffn)\n",
    "        ffn_output = layers.Dropout(self.dropout)(ffn)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "    def _prepare_data(self, residuals_dict):\n",
    "        I_res, R_res, D_res = residuals_dict['I'], residuals_dict['R'], residuals_dict['D']\n",
    "        I_scaled = self.scalers['I'].fit_transform(I_res.reshape(-1, 1)).flatten()\n",
    "        R_scaled = self.scalers['R'].fit_transform(R_res.reshape(-1, 1)).flatten()\n",
    "        D_scaled = self.scalers['D'].fit_transform(D_res.reshape(-1, 1)).flatten()\n",
    "        X, y = [], []\n",
    "        n_samples = len(I_scaled)\n",
    "        for i in range(n_samples - self.seq_length - self.forecast_horizon + 1):\n",
    "            X.append(np.column_stack([I_scaled[i:i+self.seq_length], R_scaled[i:i+self.seq_length], D_scaled[i:i+self.seq_length]]))\n",
    "            y.append(np.column_stack([I_scaled[i+self.seq_length:i+self.seq_length+self.forecast_horizon],\n",
    "                                      R_scaled[i+self.seq_length:i+self.seq_length+self.forecast_horizon],\n",
    "                                      D_scaled[i+self.seq_length:i+self.seq_length+self.forecast_horizon]]))\n",
    "        if X and y:\n",
    "            return np.array(X).reshape(-1, self.seq_length, 3), np.array(y).reshape(-1, self.forecast_horizon, 3)\n",
    "        else:\n",
    "            print(\"Za mało danych do utworzenia sekwencji!\")\n",
    "            return None, None\n",
    "\n",
    "    def fit(self, residuals_dict, validation_split=0.2, epochs=100, verbose=1):\n",
    "        print(\"Przygotowywanie danych do treningu modelu transformatora...\")\n",
    "        _ = self._analyze_noise_level(residuals_dict)\n",
    "        X, y = self._prepare_data(residuals_dict)\n",
    "        if X is None or y is None or len(X) < 10:\n",
    "            print(\"Za mało danych do wytrenowania modelu transformatora!\")\n",
    "            self.is_trained = False\n",
    "            return None\n",
    "        print(f\"Dane treningowe: {X.shape}, Etykiety: {y.shape}\")\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "                     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-6)]\n",
    "        print(\"Rozpoczęcie trenowania modelu transformatora...\")\n",
    "        history = self.model.fit(X, y, validation_split=validation_split, epochs=epochs,\n",
    "                                 batch_size=self.batch_size, callbacks=callbacks, verbose=verbose)\n",
    "        self.is_trained = True\n",
    "        print(\"Zakończono trenowanie modelu transformatora.\")\n",
    "        return history\n",
    "\n",
    "    def generate(self, residuals_dict, n_steps, last_observations=None):\n",
    "        if not self.is_trained:\n",
    "            print(\"Model nie został wytrenowany! Nie można generować prognoz.\")\n",
    "            print(\"Używam zapasowego generatora residuów.\")\n",
    "            return self._fallback_generator(residuals_dict, n_steps)\n",
    "        I_res, R_res, D_res = residuals_dict['I'], residuals_dict['R'], residuals_dict['D']\n",
    "        if last_observations is not None and len(last_observations) >= self.seq_length * 3:\n",
    "            seq_data = np.array(last_observations[-self.seq_length*3:]).reshape(self.seq_length, 3)\n",
    "        else:\n",
    "            seq_data = np.column_stack([I_res[-self.seq_length:], R_res[-self.seq_length:], D_res[-self.seq_length:]])\n",
    "        seq_norm = np.zeros_like(seq_data)\n",
    "        seq_norm[:, 0] = self.scalers['I'].transform(seq_data[:, 0].reshape(-1, 1)).flatten()\n",
    "        seq_norm[:, 1] = self.scalers['R'].transform(seq_data[:, 1].reshape(-1, 1)).flatten()\n",
    "        seq_norm[:, 2] = self.scalers['D'].transform(seq_data[:, 2].reshape(-1, 1)).flatten()\n",
    "        gen_res = []\n",
    "        rem_steps = n_steps\n",
    "        curr_input = seq_norm.reshape(1, self.seq_length, 3)\n",
    "        while rem_steps > 0:\n",
    "            steps_pred = min(rem_steps, self.forecast_horizon)\n",
    "            pred_seq = self.model.predict(curr_input, verbose=0)[0, :steps_pred, :]\n",
    "            for step in range(steps_pred):\n",
    "                i = self.scalers['I'].inverse_transform(pred_seq[step, 0].reshape(-1, 1))[0, 0]\n",
    "                r = self.scalers['R'].inverse_transform(pred_seq[step, 1].reshape(-1, 1))[0, 0]\n",
    "                d = self.scalers['D'].inverse_transform(pred_seq[step, 2].reshape(-1, 1))[0, 0]\n",
    "                gen_res.append([i, r, d])\n",
    "            if steps_pred < self.seq_length:\n",
    "                curr_input = np.concatenate([curr_input[0, steps_pred:, :], pred_seq]).reshape(1, self.seq_length, 3)\n",
    "            else:\n",
    "                curr_input = pred_seq[-self.seq_length:, :].reshape(1, self.seq_length, 3)\n",
    "            rem_steps -= steps_pred\n",
    "        gen_res = np.array(gen_res)\n",
    "        return {'I': gen_res[:, 0], 'R': gen_res[:, 1], 'D': gen_res[:, 2]}\n",
    "\n",
    "    def _fallback_generator(self, residuals_dict, n_steps):\n",
    "        print(\"Używam zapasowego generatora residuów - AR(1) z szumem.\")\n",
    "        I_res, R_res, D_res = residuals_dict['I'], residuals_dict['R'], residuals_dict['D']\n",
    "        I_m, I_s = np.mean(I_res), np.std(I_res)\n",
    "        R_m, R_s = np.mean(R_res), np.std(R_res)\n",
    "        D_m, D_s = np.mean(D_res), np.std(D_res)\n",
    "        I_new = np.random.normal(I_m, I_s, n_steps)\n",
    "        R_new = np.random.normal(R_m, R_s, n_steps)\n",
    "        D_new = np.random.normal(D_m, D_s, n_steps)\n",
    "        for i in range(1, n_steps):\n",
    "            I_new[i] = 0.7 * I_new[i-1] + 0.3 * np.random.normal(I_m, I_s)\n",
    "            R_new[i] = 0.8 * R_new[i-1] + 0.2 * np.random.normal(R_m, R_s)\n",
    "            D_new[i] = 0.9 * D_new[i-1] + 0.1 * np.random.normal(D_m, D_s)\n",
    "        return {'I': I_new, 'R': R_new, 'D': D_new}\n",
    "\n",
    "    def plot_history(self, history):\n",
    "        if history is None:\n",
    "            return\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        if 'lr' in history.history:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['lr'])\n",
    "            plt.title('Learning Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f27a7",
   "metadata": {},
   "source": [
    "### 3.3.1 Ulepszony Model Łańcuchów Markowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc243d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMarkovChainResiduals:\n",
    "    def __init__(self, n_states=7, memory_length=5, random_state=42):\n",
    "        self.n_states = n_states\n",
    "        self.memory_length = memory_length\n",
    "        self.random_state = random_state\n",
    "        self.kmeans = KMeans(n_clusters=n_states, random_state=random_state, n_init=10)\n",
    "        self.transition_matrix = None\n",
    "        self.state_distributions = None\n",
    "        self.initial_distribution = None\n",
    "        self.use_pca = False\n",
    "        self.pca = None\n",
    "        self.memory_models = []\n",
    "        self.use_memory_models = False\n",
    "\n",
    "    def fit(self, residuals_dict):\n",
    "        print(\"Trenowanie modelu Markowa na residuach...\")\n",
    "        I_res, R_res, D_res = residuals_dict['I'], residuals_dict['R'], residuals_dict['D']\n",
    "        if len(I_res) < self.memory_length + 5:\n",
    "            print(f\"Za malo danych ({len(I_res)} dla pamięci = {self.memory_length}). Redukuje pamięć.\")\n",
    "            self.memory_length = max(1, len(I_res) // 3)\n",
    "        data, temporal_features = [], []\n",
    "        for i in range(self.memory_length, len(I_res)):\n",
    "            curr = [I_res[i], R_res[i], D_res[i]]\n",
    "            for lag in range(1, min(self.memory_length + 1, 4)):\n",
    "                curr.extend([I_res[i] - I_res[i-lag], R_res[i] - R_res[i-lag], D_res[i] - D_res[i-lag]])\n",
    "            mem = []\n",
    "            for j in range(1, self.memory_length + 1):\n",
    "                mem.extend([I_res[i-j], R_res[i-j], D_res[i-j]])\n",
    "            data.append(curr)\n",
    "            temporal_features.append(mem)\n",
    "\n",
    "        if len(data) < 10:\n",
    "            data = np.column_stack([I_res, R_res, D_res])\n",
    "            states = self.kmeans.fit_predict(data)\n",
    "            temporal_features = []\n",
    "            self.use_pca = False\n",
    "        else:\n",
    "            data = np.array(data)\n",
    "            temporal_features = np.array(temporal_features) if temporal_features else np.array([])\n",
    "            if data.shape[1] > 10:\n",
    "                try:\n",
    "                    self.pca = PCA(n_components=min(10, data.shape[1]), random_state=self.random_state)\n",
    "                    data_reduced = self.pca.fit_transform(data)\n",
    "                    print(f\"Zredukowano wymiarowosc z {data.shape[1]} do {data_reduced.shape[1]} za pomocą PCA.\")\n",
    "                    states = self.kmeans.fit_predict(data_reduced)\n",
    "                    self.use_pca = True\n",
    "                except Exception as e:\n",
    "                    print(f\"{e}. Uzywane beda oryginalne zmienne.\")\n",
    "                    states = self.kmeans.fit_predict(data)\n",
    "                    self.use_pca = False\n",
    "            else:\n",
    "                states = self.kmeans.fit_predict(data)\n",
    "                self.use_pca = False\n",
    "\n",
    "        self.transition_matrix = np.zeros((self.n_states, self.n_states))\n",
    "        if len(states) > 1:\n",
    "            for i in range(len(states) - 1):\n",
    "                self.transition_matrix[states[i], states[i+1]] += 1\n",
    "            row_sums = self.transition_matrix.sum(axis=1)\n",
    "            for i in range(self.n_states):\n",
    "                if row_sums[i] > 0:\n",
    "                    self.transition_matrix[i, :] /= row_sums[i]\n",
    "        else:\n",
    "            self.transition_matrix = np.ones((self.n_states, self.n_states)) / self.n_states\n",
    "\n",
    "        self.initial_distribution = np.zeros(self.n_states)\n",
    "        for s in states:\n",
    "            self.initial_distribution[s] += 1\n",
    "        self.initial_distribution = self.initial_distribution / len(states) if len(states) > 0 else np.ones(self.n_states) / self.n_states\n",
    "\n",
    "        self.state_distributions = []\n",
    "        default_dim = data.shape[1] if data.ndim > 1 else 3\n",
    "        for state in range(self.n_states):\n",
    "            idx = np.where(states == state)[0]\n",
    "            if len(idx) > 1:\n",
    "                state_data = data[idx]\n",
    "                mean = np.mean(state_data, axis=0)\n",
    "                try:\n",
    "                    cov = np.cov(state_data, rowvar=False)\n",
    "                    if np.isscalar(cov) or cov.ndim == 0: cov = np.array([[max(cov, 1e-6)]])\n",
    "                    elif cov.ndim == 1: cov = np.diag(np.maximum(cov, 1e-6))\n",
    "                    else: cov = np.maximum(cov, 1e-6 * np.eye(cov.shape[0]))\n",
    "                except Exception as e:\n",
    "                    print(f\"Covariance error state {state}: {e}\")\n",
    "                    var = np.var(state_data, axis=0)\n",
    "                    cov = np.diag(np.maximum(var, 1e-6))\n",
    "                self.state_distributions.append((mean, cov))\n",
    "            elif len(idx) == 1:\n",
    "                mean = data[idx[0]]\n",
    "                var_scale = 0.01 * np.mean(np.abs(mean)) + 1e-6\n",
    "                cov = np.eye(mean.shape[0]) * var_scale\n",
    "                self.state_distributions.append((mean, cov))\n",
    "            else:\n",
    "                mean = np.zeros(default_dim)\n",
    "                cov = np.eye(default_dim) * 0.01\n",
    "                self.state_distributions.append((mean, cov))\n",
    "                print(f\"Warning: No samples for state {state}\")\n",
    "\n",
    "        self.use_memory_models = False\n",
    "        if len(temporal_features) > 0 and len(states) > 5:\n",
    "            try:\n",
    "                self.memory_models = []\n",
    "                valid_models = 0\n",
    "                for state in range(self.n_states):\n",
    "                    prev_idx = np.where(states[:-1] == state)[0]\n",
    "                    if len(prev_idx) > 5:\n",
    "                        X = temporal_features[prev_idx]\n",
    "                        y = states[1:][prev_idx]\n",
    "                        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500, C=0.8, random_state=self.random_state)\n",
    "                        try:\n",
    "                            model.fit(X, y)\n",
    "                            self.memory_models.append(model)\n",
    "                            valid_models += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error fitting memory model state {state}: {e}\")\n",
    "                            self.memory_models.append(None)\n",
    "                    else:\n",
    "                        self.memory_models.append(None)\n",
    "                print(f\"Trained {valid_models}/{self.n_states} memory models\")\n",
    "                self.use_memory_models = valid_models > 0\n",
    "            except Exception as e:\n",
    "                print(f\"Error setting up memory models: {e}\")\n",
    "                self.use_memory_models = False\n",
    "\n",
    "    def generate(self, n_steps, last_state=None, last_observations=None):\n",
    "        if self.initial_distribution is None:\n",
    "             print(\"Model Markowa nie został wytrenowany. Zwracam zera.\")\n",
    "             zeros = np.zeros(n_steps)\n",
    "             return {'I': zeros, 'R': zeros, 'D': zeros}\n",
    "\n",
    "        current_state = np.random.choice(self.n_states, p=self.initial_distribution) if last_state is None else last_state\n",
    "        gen_res = []\n",
    "        mem_win = np.array(last_observations).flatten() if last_observations is not None and self.use_memory_models else None\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            use_mem = (mem_win is not None and self.use_memory_models and\n",
    "                       current_state < len(self.memory_models) and self.memory_models[current_state] is not None)\n",
    "            next_state = -1\n",
    "            if use_mem:\n",
    "                try:\n",
    "                    expected_features = self.memory_models[current_state].n_features_in_\n",
    "                    if mem_win.shape[0] == expected_features:\n",
    "                        probs = self.memory_models[current_state].predict_proba(mem_win.reshape(1, -1))[0]\n",
    "                        next_state = np.random.choice(self.n_states, p=probs)\n",
    "                    else:\n",
    "                         print(f\"Ostrzeżenie: Niezgodność wymiarów pamięci ({mem_win.shape[0]} vs {expected_features}). Używam standardowej macierzy przejść.\")\n",
    "                         use_mem = False\n",
    "                except Exception as e:\n",
    "                    print(f\"{e}\")\n",
    "                    use_mem = False\n",
    "            if not use_mem or next_state == -1:\n",
    "                if current_state >= self.transition_matrix.shape[0] or np.sum(self.transition_matrix[current_state]) < 1e-9:\n",
    "                    next_state = np.random.choice(self.n_states)\n",
    "                else:\n",
    "                    probs = self.transition_matrix[current_state]\n",
    "                    probs = probs / (np.sum(probs) + 1e-9)\n",
    "                    next_state = np.random.choice(self.n_states, p=probs)\n",
    "\n",
    "            next_state = min(next_state, self.n_states - 1)\n",
    "            target_dim = 3\n",
    "            if self.state_distributions and next_state < len(self.state_distributions):\n",
    "                 mean, cov = self.state_distributions[next_state]\n",
    "            else:\n",
    "                 print(f\"Ostrzeżenie: Brak dystrybucji dla stanu {next_state}. Używam zer.\")\n",
    "                 mean, cov = np.zeros(target_dim), np.eye(target_dim) * 1e-6\n",
    "\n",
    "            if mean.shape[0] > target_dim: mean = mean[:target_dim]\n",
    "            if cov.ndim == 2 and cov.shape[0] > target_dim: cov = cov[:target_dim, :target_dim]\n",
    "            elif cov.ndim == 1 and cov.shape[0] > target_dim: cov = cov[:target_dim]\n",
    "            if mean.shape[0] < target_dim: mean = np.pad(mean, (0, target_dim - mean.shape[0]))\n",
    "            if cov.ndim == 2 and cov.shape[0] < target_dim: cov = np.pad(cov, ((0, target_dim - cov.shape[0]), (0, target_dim - cov.shape[0])))\n",
    "            elif cov.ndim == 1 and cov.shape[0] < target_dim: cov = np.pad(cov, (0, target_dim - cov.shape[0]))\n",
    "            if cov.ndim == 1: cov = np.diag(np.maximum(cov, 1e-6))\n",
    "            elif np.isscalar(cov): cov = np.eye(target_dim) * max(cov, 1e-6)\n",
    "\n",
    "            try:\n",
    "                if cov.ndim == 2 and cov.shape[0] > 1 and not np.all(np.linalg.eigvals(cov) >= -1e-9):\n",
    "                    cov = cov + np.eye(cov.shape[0]) * max(1e-6, 0.01 * np.trace(cov) / cov.shape[0])\n",
    "            except Exception as e:\n",
    "                print(f\"{e}\")\n",
    "                cov = np.diag(np.diag(cov))\n",
    "\n",
    "            try:\n",
    "                residual = multivariate_normal.rvs(mean=mean, cov=cov, size=1, random_state=self.random_state).flatten()\n",
    "                if residual.shape[0] < target_dim: residual = np.pad(residual, (0, target_dim - residual.shape[0]))\n",
    "                elif residual.shape[0] > target_dim: residual = residual[:target_dim]\n",
    "                gen_res.append(residual)\n",
    "                if mem_win is not None:\n",
    "                    new_obs = residual[:target_dim]\n",
    "                    mem_win = np.concatenate([new_obs, mem_win[:-target_dim]])\n",
    "            except Exception as e:\n",
    "                print(f\"Błąd generowania residuów: {e}\")\n",
    "                diag_cov = np.diag(cov) if cov.ndim == 2 else cov\n",
    "                simple_residual = mean + np.random.normal(0, 1, size=mean.shape) * np.sqrt(np.maximum(diag_cov, 1e-6))\n",
    "                if simple_residual.shape[0] < target_dim: simple_residual = np.pad(simple_residual, (0, target_dim - simple_residual.shape[0]))\n",
    "                elif simple_residual.shape[0] > target_dim: simple_residual = simple_residual[:target_dim]\n",
    "                gen_res.append(simple_residual)\n",
    "                if mem_win is not None:\n",
    "                    new_obs = simple_residual[:target_dim]\n",
    "                    mem_win = np.concatenate([new_obs, mem_win[:-target_dim]])\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "        gen_res = np.array(gen_res)\n",
    "        if gen_res.ndim == 1: gen_res = gen_res.reshape(-1, 1)\n",
    "        if gen_res.shape[1] < 3:\n",
    "             padded = np.zeros((gen_res.shape[0], 3))\n",
    "             padded[:, :gen_res.shape[1]] = gen_res\n",
    "             gen_res = padded\n",
    "\n",
    "        return {'I': gen_res[:, 0], 'R': gen_res[:, 1], 'D': gen_res[:, 2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6a7c4",
   "metadata": {},
   "source": [
    "### 3.4 Funkcje Pomocnicze (ekstrakcja residuów, korekcja trendu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281227a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_residuals(S_simulations, I_simulations, R_simulations, D_simulations,\n",
    "                      df, start_date, end_date):\n",
    "    df_period = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    df_period.reset_index(drop=True, inplace=True)\n",
    "    population = POPULATION\n",
    "    S_emp = population - (df_period[\"Active\"] + df_period[\"Recovered\"] + df_period[\"Deaths\"]).values\n",
    "    I_emp = df_period[\"Active\"].values\n",
    "    R_emp = df_period[\"Recovered\"].values\n",
    "    D_emp = df_period[\"Deaths\"].values\n",
    "    S_array, I_array, R_array, D_array = map(np.array, [S_simulations, I_simulations, R_simulations, D_simulations])\n",
    "    train_len = len(df_period)\n",
    "    if I_array.shape[0] == 0 or I_array.shape[1] < train_len:\n",
    "         print(\"Ostrzeżenie: Brak wystarczających danych symulacyjnych do ekstrakcji residuów.\")\n",
    "         zeros = np.zeros(train_len)\n",
    "         return {'S': zeros, 'I': zeros, 'R': zeros, 'D': zeros}\n",
    "    S_median = np.median(S_array[:, :train_len], axis=0)\n",
    "    I_median = np.median(I_array[:, :train_len], axis=0)\n",
    "    R_median = np.median(R_array[:, :train_len], axis=0)\n",
    "    D_median = np.median(D_array[:, :train_len], axis=0)\n",
    "    S_residuals = S_emp - S_median\n",
    "    I_residuals = I_emp - I_median\n",
    "    R_residuals = R_emp - R_median\n",
    "    D_residuals = D_emp - D_median\n",
    "    return {'S': S_residuals, 'I': I_residuals, 'R': R_residuals, 'D': D_residuals}\n",
    "\n",
    "def apply_trend_correction(params, df, start_date, end_date, forecast_days):\n",
    "    df_last = df[(df[\"Last_Update\"] > end_date - pd.Timedelta(days=7)) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    if len(df_last) >= 5:\n",
    "        I_val = df_last[\"Active\"].values\n",
    "        R_val = df_last[\"Recovered\"].values\n",
    "        D_val = df_last[\"Deaths\"].values\n",
    "        I_ch = np.mean(np.diff(I_val)) / (np.mean(I_val) + 1e-6)\n",
    "        R_ch = np.mean(np.diff(R_val)) / (np.mean(R_val) + 1e-6)\n",
    "        D_ch = np.mean(np.diff(D_val)) / (np.mean(D_val) + 1e-6)\n",
    "        corr_p = params.copy()\n",
    "        if I_ch < -0.01:\n",
    "            corr_p[\"beta2\"] *= max(0.7, 1.0 + 2 * I_ch)\n",
    "        if R_ch > 0.01:\n",
    "            corr_p[\"gamma\"] *= min(1.3, 1.0 + R_ch)\n",
    "        if D_ch > 0.01:\n",
    "            corr_p[\"mu\"] *= min(1.2, 1.0 + D_ch)\n",
    "        print(f\"Trend korekcja: beta2 {params['beta2']:.4f}->{corr_p['beta2']:.4f}, gamma {params['gamma']:.4f}->{corr_p['gamma']:.4f}, mu {params['mu']:.4f}->{corr_p['mu']:.4f}\")\n",
    "        return corr_p\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformer_markov_hybrid_correction(S_simulations, I_simulations, R_simulations, D_simulations,\n",
    "                                     df, start_date, end_date, forecast_days,\n",
    "                                     transformer_weight=0.6):\n",
    "    \"\"\"Łączy metody korekcji residuów. (Poprawiono formatowanie)\"\"\"\n",
    "    print(\"Stosowanie hybrydowej korekcji residuów (transformer + Markov)...\")\n",
    "    residuals = extract_residuals(S_simulations, I_simulations, R_simulations, D_simulations, df, start_date, end_date)\n",
    "    transformer_residuals = None\n",
    "    try:\n",
    "        min_seq_length = min(5, len(residuals['I']))\n",
    "        print(f\"Próba treningu transformatora z długością sekwencji {min_seq_length}...\")\n",
    "        transformer_model = TransformerResidualModel(seq_length=min_seq_length, forecast_horizon=forecast_days)\n",
    "        transformer_history = transformer_model.fit(residuals, epochs=150, validation_split=0.2, verbose=0)\n",
    "        if transformer_history is not None and transformer_model.is_trained:\n",
    "            print(\"Transformator wytrenowany pomyślnie!\")\n",
    "            memory_window = []\n",
    "            for i in range(min(transformer_model.seq_length, len(residuals['I']))):\n",
    "                day = len(residuals['I']) - 1 - i\n",
    "                memory_window.extend([residuals['I'][day], residuals['R'][day], residuals['D'][day]])\n",
    "            transformer_residuals = transformer_model.generate(residuals, forecast_days, memory_window)\n",
    "        else:\n",
    "            print(\"Nie udało się wytrenować transformatora.\")\n",
    "            transformer_weight = 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd podczas trenowania/generowania transformatora: {e}\")\n",
    "        transformer_weight = 0.0\n",
    "\n",
    "    print(\"Trenowanie modelu Markowa...\")\n",
    "    mem_len_markov = min(5, len(residuals['I'])-1 if len(residuals['I']) > 1 else 1)\n",
    "    markov_model = EnhancedMarkovChainResiduals(n_states=9, memory_length=mem_len_markov)\n",
    "    markov_model.fit(residuals)\n",
    "    last_residuals = np.array([residuals['I'][-1], residuals['R'][-1], residuals['D'][-1]]).reshape(1, -1)\n",
    "    memory_window_markov = []\n",
    "    for i in range(mem_len_markov):\n",
    "        day = len(residuals['I']) - 2 - i\n",
    "        if day >= 0:\n",
    "            memory_window_markov.extend([residuals['I'][day], residuals['R'][day], residuals['D'][day]])\n",
    "\n",
    "    last_state = 0 # Domyślny stan\n",
    "    if hasattr(markov_model, 'kmeans') and hasattr(markov_model.kmeans, 'cluster_centers_'):\n",
    "        if hasattr(markov_model, 'use_pca') and markov_model.use_pca and hasattr(markov_model, 'pca'):\n",
    "            expected_dim = markov_model.pca.n_features_in_\n",
    "            current_dim = last_residuals.shape[1]\n",
    "            padding_needed = expected_dim - current_dim\n",
    "            if padding_needed < 0: padding_needed = 0\n",
    "            last_features_padded = np.concatenate([last_residuals.flatten(), np.zeros(padding_needed)])\n",
    "            if last_features_padded.shape[0] == expected_dim:\n",
    "                 try:\n",
    "                     last_features_pca = markov_model.pca.transform(last_features_padded.reshape(1, -1))\n",
    "                     last_state = markov_model.kmeans.predict(last_features_pca)[0]\n",
    "                 except Exception as e:\n",
    "                     print(f\"Błąd predykcji PCA w hybrydzie: {e}. Używam standardowej.\")\n",
    "                     last_state = markov_model.kmeans.predict(last_residuals)[0]\n",
    "            else:\n",
    "                 print(f\"Ostrzeżenie: Niezgodność wymiarów PCA w hybrydzie ({last_features_padded.shape[0]} vs {expected_dim}). Używam standardowej.\")\n",
    "                 last_state = markov_model.kmeans.predict(last_residuals)[0]\n",
    "        else:\n",
    "            last_state = markov_model.kmeans.predict(last_residuals)[0]\n",
    "    else:\n",
    "         print(\"Ostrzeżenie: Model Markowa (kmeans) nie zainicjalizowany w hybrydzie.\")\n",
    "\n",
    "    markov_residuals = markov_model.generate(forecast_days, last_state, memory_window_markov)\n",
    "\n",
    "    combined_residuals = {'I': np.zeros(forecast_days), 'R': np.zeros(forecast_days), 'D': np.zeros(forecast_days)}\n",
    "    if transformer_residuals is not None:\n",
    "        for key in combined_residuals.keys():\n",
    "            combined_residuals[key] = transformer_weight * transformer_residuals[key] + (1 - transformer_weight) * markov_residuals[key]\n",
    "        print(f\"Połączono residua z transformatora (waga {transformer_weight:.2f}) i modelu Markowa (waga {1-transformer_weight:.2f}).\")\n",
    "    else:\n",
    "        combined_residuals = markov_residuals\n",
    "        print(\"Użyto tylko residuów z modelu Markowa.\")\n",
    "\n",
    "    S_corrected, I_corrected, R_corrected, D_corrected = [], [], [], []\n",
    "    S_array, I_array, R_array, D_array = map(np.array, [S_simulations, I_simulations, R_simulations, D_simulations])\n",
    "    train_days = (end_date - start_date).days + 1\n",
    "    decay_factor = np.exp(-np.arange(forecast_days) / 14)\n",
    "\n",
    "    for i in range(len(I_simulations)):\n",
    "        noise_factor = 0.15\n",
    "        i_std = np.std(combined_residuals['I'])\n",
    "        r_std = np.std(combined_residuals['R'])\n",
    "        d_std = np.std(combined_residuals['D'])\n",
    "        i_noise = np.random.normal(0, noise_factor * i_std) if i_std > 1e-9 else 0\n",
    "        r_noise = np.random.normal(0, noise_factor * r_std) if r_std > 1e-9 else 0\n",
    "        d_noise = np.random.normal(0, noise_factor * d_std) if d_std > 1e-9 else 0\n",
    "\n",
    "        if forecast_days > 0:\n",
    "            scale_i = min(1.0, 0.7 + np.random.uniform(-0.2, 0.2))\n",
    "            scale_r = min(1.0, 0.6 + np.random.uniform(-0.2, 0.2))\n",
    "            scale_d = min(1.0, 0.65 + np.random.uniform(-0.2, 0.2))\n",
    "            forecast_len = min(forecast_days, I_array.shape[1] - train_days)\n",
    "            if forecast_len <= 0:\n",
    "                 I_sim = I_array[i, :].copy()\n",
    "                 R_sim = R_array[i, :].copy()\n",
    "                 D_sim = D_array[i, :].copy()\n",
    "            else:\n",
    "                I_forecast = I_array[i, train_days:train_days+forecast_len].copy()\n",
    "                R_forecast = R_array[i, train_days:train_days+forecast_len].copy()\n",
    "                D_forecast = D_array[i, train_days:train_days+forecast_len].copy()\n",
    "                actual_forecast_days = min(forecast_len, len(combined_residuals['I']), len(decay_factor))\n",
    "                for day in range(actual_forecast_days):\n",
    "                    I_forecast[day] += scale_i * (combined_residuals['I'][day] + i_noise) * decay_factor[day]\n",
    "                    R_forecast[day] += scale_r * (combined_residuals['R'][day] + r_noise) * decay_factor[day]\n",
    "                    D_forecast[day] += scale_d * (combined_residuals['D'][day] + d_noise) * decay_factor[day]\n",
    "                I_forecast = np.maximum(I_forecast, 0)\n",
    "                R_forecast = np.maximum(R_forecast, 0)\n",
    "                D_forecast = np.maximum(D_forecast, 0)\n",
    "                I_sim = np.concatenate([I_array[i, :train_days], I_forecast])\n",
    "                R_sim = np.concatenate([R_array[i, :train_days], R_forecast])\n",
    "                D_sim = np.concatenate([D_array[i, :train_days], D_forecast])\n",
    "        else:\n",
    "            I_sim = I_array[i, :].copy()\n",
    "            R_sim = R_array[i, :].copy()\n",
    "            D_sim = D_array[i, :].copy()\n",
    "        I_corrected.append(I_sim)\n",
    "        R_corrected.append(R_sim)\n",
    "        D_corrected.append(D_sim)\n",
    "\n",
    "    for i in range(len(S_simulations)):\n",
    "        s_len = min(S_array.shape[1], len(I_corrected[i]))\n",
    "        old_sum = I_array[i, :s_len] + R_array[i, :s_len] + D_array[i, :s_len]\n",
    "        new_sum = I_corrected[i][:s_len] + R_corrected[i][:s_len] + D_corrected[i][:s_len]\n",
    "        S_sim = S_array[i, :s_len] - (new_sum - old_sum)\n",
    "        S_corrected.append(S_sim)\n",
    "\n",
    "    print(\"Hybrydowa korekcja residuów zakończona.\")\n",
    "    return S_corrected, I_corrected, R_corrected, D_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afe8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformer_residuals_correction(S_simulations, I_simulations, R_simulations, D_simulations,\n",
    "                                         df, start_date, end_date, forecast_days):\n",
    "    \"\"\"Stosuje korekcję residuów z transformatora. (Poprawiono formatowanie)\"\"\"\n",
    "    print(\"Stosowanie korekcji residuów z użyciem modelu transformatora...\")\n",
    "    residuals = extract_residuals(S_simulations, I_simulations, R_simulations, D_simulations, df, start_date, end_date)\n",
    "    transformer_model = TransformerResidualModel(seq_length=min(14, len(residuals['I'])), forecast_horizon=forecast_days)\n",
    "    history = transformer_model.fit(residuals, epochs=200, validation_split=0.2, verbose=0)\n",
    "\n",
    "    if not transformer_model.is_trained:\n",
    "         print(\"Model transformatora nie został wytrenowany. Pomijam korekcję.\")\n",
    "         return S_simulations, I_simulations, R_simulations, D_simulations\n",
    "\n",
    "    memory_window = []\n",
    "    for i in range(min(transformer_model.seq_length, len(residuals['I']))):\n",
    "        day = len(residuals['I']) - 1 - i\n",
    "        memory_window.extend([residuals['I'][day], residuals['R'][day], residuals['D'][day]])\n",
    "    forecast_residuals = transformer_model.generate(residuals, forecast_days, memory_window)\n",
    "\n",
    "    S_corrected, I_corrected, R_corrected, D_corrected = [], [], [], []\n",
    "    S_array, I_array, R_array, D_array = map(np.array, [S_simulations, I_simulations, R_simulations, D_simulations])\n",
    "    train_days = (end_date - start_date).days + 1\n",
    "    decay_factor = np.exp(-np.arange(forecast_days) / 15)\n",
    "    correction_scales = {'I': 0.7, 'R': 0.5, 'D': 0.6}\n",
    "\n",
    "    for i in range(len(I_simulations)):\n",
    "        noise_factor = 0.15\n",
    "        i_std = np.std(forecast_residuals['I'])\n",
    "        r_std = np.std(forecast_residuals['R'])\n",
    "        d_std = np.std(forecast_residuals['D'])\n",
    "        i_noise = np.random.normal(0, noise_factor * i_std) if i_std > 1e-9 else 0\n",
    "        r_noise = np.random.normal(0, noise_factor * r_std) if r_std > 1e-9 else 0\n",
    "        d_noise = np.random.normal(0, noise_factor * d_std) if d_std > 1e-9 else 0\n",
    "\n",
    "        if forecast_days > 0:\n",
    "            i_scale = correction_scales['I'] * (1.0 + np.random.uniform(-0.1, 0.1))\n",
    "            r_scale = correction_scales['R'] * (1.0 + np.random.uniform(-0.1, 0.1))\n",
    "            d_scale = correction_scales['D'] * (1.0 + np.random.uniform(-0.1, 0.1))\n",
    "            forecast_len = min(forecast_days, I_array.shape[1] - train_days)\n",
    "            if forecast_len <= 0:\n",
    "                 I_sim = I_array[i, :].copy()\n",
    "                 R_sim = R_array[i, :].copy()\n",
    "                 D_sim = D_array[i, :].copy()\n",
    "            else:\n",
    "                I_forecast = I_array[i, train_days:train_days+forecast_len].copy()\n",
    "                R_forecast = R_array[i, train_days:train_days+forecast_len].copy()\n",
    "                D_forecast = D_array[i, train_days:train_days+forecast_len].copy()\n",
    "                actual_forecast_days = min(forecast_len, len(forecast_residuals['I']), len(decay_factor))\n",
    "                for day in range(actual_forecast_days):\n",
    "                    I_forecast[day] += i_scale * (forecast_residuals['I'][day] + i_noise) * decay_factor[day]\n",
    "                    R_forecast[day] += r_scale * (forecast_residuals['R'][day] + r_noise) * decay_factor[day]\n",
    "                    D_forecast[day] += d_scale * (forecast_residuals['D'][day] + d_noise) * decay_factor[day]\n",
    "                I_forecast = np.maximum(I_forecast, 0)\n",
    "                R_forecast = np.maximum(R_forecast, 0)\n",
    "                D_forecast = np.maximum(D_forecast, 0)\n",
    "                I_sim = np.concatenate([I_array[i, :train_days], I_forecast])\n",
    "                R_sim = np.concatenate([R_array[i, :train_days], R_forecast])\n",
    "                D_sim = np.concatenate([D_array[i, :train_days], D_forecast])\n",
    "        else:\n",
    "            I_sim = I_array[i, :].copy()\n",
    "            R_sim = R_array[i, :].copy()\n",
    "            D_sim = D_array[i, :].copy()\n",
    "        I_corrected.append(I_sim)\n",
    "        R_corrected.append(R_sim)\n",
    "        D_corrected.append(D_sim)\n",
    "\n",
    "    for i in range(len(S_simulations)):\n",
    "        s_len = min(S_array.shape[1], len(I_corrected[i]))\n",
    "        old_sum = I_array[i, :s_len] + R_array[i, :s_len] + D_array[i, :s_len]\n",
    "        new_sum = I_corrected[i][:s_len] + R_corrected[i][:s_len] + D_corrected[i][:s_len]\n",
    "        S_sim = S_array[i, :s_len] - (new_sum - old_sum)\n",
    "        S_corrected.append(S_sim)\n",
    "\n",
    "    return S_corrected, I_corrected, R_corrected, D_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3190d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_enhanced_markov_residuals_correction(S_simulations, I_simulations, R_simulations, D_simulations,\n",
    "                                    df, start_date, end_date, forecast_days):\n",
    "    \"\"\"Korekcja residuów z ulepszonym Markowem. (Poprawiono formatowanie)\"\"\"\n",
    "    print(\"Applying enhanced Markov chain residual correction...\")\n",
    "    residuals = extract_residuals(S_simulations, I_simulations, R_simulations, D_simulations, df, start_date, end_date)\n",
    "    if not residuals or np.all(residuals['I'] == 0):\n",
    "         print(\"Ostrzeżenie: Puste lub zerowe residua. Pomijam korekcję Markowa.\")\n",
    "         return S_simulations, I_simulations, R_simulations, D_simulations\n",
    "\n",
    "    mem_len_markov = min(5, len(residuals['I'])-1 if len(residuals['I']) > 1 else 1)\n",
    "    markov_model = EnhancedMarkovChainResiduals(n_states=9, memory_length=mem_len_markov)\n",
    "    markov_model.fit(residuals)\n",
    "    last_residuals = np.array([residuals['I'][-1], residuals['R'][-1], residuals['D'][-1]]).reshape(1, -1)\n",
    "    memory_window_markov = []\n",
    "    for i in range(mem_len_markov):\n",
    "        day = len(residuals['I']) - 2 - i\n",
    "        if day >= 0:\n",
    "            memory_window_markov.extend([residuals['I'][day], residuals['R'][day], residuals['D'][day]])\n",
    "\n",
    "    last_state = 0\n",
    "    if hasattr(markov_model, 'kmeans') and hasattr(markov_model.kmeans, 'cluster_centers_'):\n",
    "        if hasattr(markov_model, 'use_pca') and markov_model.use_pca and hasattr(markov_model, 'pca'):\n",
    "            expected_dim = markov_model.pca.n_features_in_\n",
    "            current_dim = last_residuals.shape[1]\n",
    "            padding_needed = expected_dim - current_dim\n",
    "            if padding_needed < 0: padding_needed = 0\n",
    "            last_features_padded = np.concatenate([last_residuals.flatten(), np.zeros(padding_needed)])\n",
    "            if last_features_padded.shape[0] == expected_dim:\n",
    "                 try:\n",
    "                     last_features_pca = markov_model.pca.transform(last_features_padded.reshape(1, -1))\n",
    "                     last_state = markov_model.kmeans.predict(last_features_pca)[0]\n",
    "                 except Exception as e:\n",
    "                     print(f\"Błąd predykcji PCA w Markowie: {e}. Używam standardowej.\")\n",
    "                     last_state = markov_model.kmeans.predict(last_residuals)[0]\n",
    "            else:\n",
    "                 print(f\"Ostrzeżenie: Niezgodność wymiarów PCA w Markowie ({last_features_padded.shape[0]} vs {expected_dim}). Używam standardowej.\")\n",
    "                 last_state = markov_model.kmeans.predict(last_residuals)[0]\n",
    "        else:\n",
    "            last_state = markov_model.kmeans.predict(last_residuals)[0]\n",
    "    else:\n",
    "         print(\"Ostrzeżenie: Model Markowa (kmeans) nie zainicjalizowany.\")\n",
    "\n",
    "    forecast_residuals = markov_model.generate(forecast_days, last_state, memory_window_markov)\n",
    "    S_corrected, I_corrected, R_corrected, D_corrected = [], [], [], []\n",
    "    S_array, I_array, R_array, D_array = map(np.array, [S_simulations, I_simulations, R_simulations, D_simulations])\n",
    "    train_days = (end_date - start_date).days + 1\n",
    "    decay_factor = np.exp(-np.arange(forecast_days) / 14)\n",
    "\n",
    "    for i in range(len(I_simulations)):\n",
    "        noise_factor = 0.15\n",
    "        i_std = np.std(forecast_residuals['I'])\n",
    "        r_std = np.std(forecast_residuals['R'])\n",
    "        d_std = np.std(forecast_residuals['D'])\n",
    "        i_noise = np.random.normal(0, noise_factor * i_std) if i_std > 1e-9 else 0\n",
    "        r_noise = np.random.normal(0, noise_factor * r_std) if r_std > 1e-9 else 0\n",
    "        d_noise = np.random.normal(0, noise_factor * d_std) if d_std > 1e-9 else 0\n",
    "\n",
    "        if forecast_days > 0:\n",
    "            scale_i = min(1.0, 0.6 + np.random.uniform(-0.2, 0.2))\n",
    "            scale_r = min(1.0, 0.6 + np.random.uniform(-0.2, 0.2))\n",
    "            scale_d = min(1.0, 0.6 + np.random.uniform(-0.2, 0.2))\n",
    "            forecast_len = min(forecast_days, I_array.shape[1] - train_days)\n",
    "            if forecast_len <= 0:\n",
    "                 I_sim = I_array[i, :].copy()\n",
    "                 R_sim = R_array[i, :].copy()\n",
    "                 D_sim = D_array[i, :].copy()\n",
    "            else:\n",
    "                I_forecast = I_array[i, train_days:train_days+forecast_len].copy()\n",
    "                R_forecast = R_array[i, train_days:train_days+forecast_len].copy()\n",
    "                D_forecast = D_array[i, train_days:train_days+forecast_len].copy()\n",
    "                actual_forecast_days = min(forecast_len, len(forecast_residuals['I']), len(decay_factor))\n",
    "                for day in range(actual_forecast_days):\n",
    "                    i_corr = forecast_residuals['I'][day] + i_noise\n",
    "                    r_corr = forecast_residuals['R'][day] + r_noise\n",
    "                    d_corr = forecast_residuals['D'][day] + d_noise\n",
    "                    I_forecast[day] += scale_i * i_corr * decay_factor[day]\n",
    "                    R_forecast[day] += scale_r * r_corr * decay_factor[day]\n",
    "                    D_forecast[day] += scale_d * d_corr * decay_factor[day]\n",
    "                I_forecast = np.maximum(I_forecast, 0)\n",
    "                R_forecast = np.maximum(R_forecast, 0)\n",
    "                D_forecast = np.maximum(D_forecast, 0)\n",
    "                I_sim = np.concatenate([I_array[i, :train_days], I_forecast])\n",
    "                R_sim = np.concatenate([R_array[i, :train_days], R_forecast])\n",
    "                D_sim = np.concatenate([D_array[i, :train_days], D_forecast])\n",
    "        else:\n",
    "            I_sim = I_array[i, :].copy()\n",
    "            R_sim = R_array[i, :].copy()\n",
    "            D_sim = D_array[i, :].copy()\n",
    "        I_corrected.append(I_sim)\n",
    "        R_corrected.append(R_sim)\n",
    "        D_corrected.append(D_sim)\n",
    "\n",
    "    for i in range(len(S_simulations)):\n",
    "        s_len = min(S_array.shape[1], len(I_corrected[i]))\n",
    "        old_sum = I_array[i, :s_len] + R_array[i, :s_len] + D_array[i, :s_len]\n",
    "        new_sum = I_corrected[i][:s_len] + R_corrected[i][:s_len] + D_corrected[i][:s_len]\n",
    "        S_sim = S_array[i, :s_len] - (new_sum - old_sum)\n",
    "        S_corrected.append(S_sim)\n",
    "\n",
    "    return S_corrected, I_corrected, R_corrected, D_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119bd9f",
   "metadata": {},
   "source": [
    "## 4. <mark>Komponenty Stochastyczne</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99016387",
   "metadata": {},
   "source": [
    "Rozdział ten zawiera funkcje odpowiedzialne za wprowadzenie i kwantyfikację niepewności w modelu SIRD.\n",
    "1.  **Bootstrapping parametrów:** Szacujemy, jak stabilne są parametry modelu (`beta`, `gamma`, `mu`, ...) znalezione przez PSO. Robimy to, wielokrotnie dopasowując model do lekko zmodyfikowanych (perturbowanych) danych historycznych. Rozrzut uzyskanych parametrów daje nam pojęcie o ich niepewności.\n",
    "2.  **Generowanie zróżnicowanych parametrów (KDE):** Na podstawie rozkładu parametrów z bootstrappingu, generujemy dużą liczbę nowych, \"wiarygodnych\" zestawów parametrów. Używamy do tego estymacji gęstości jądrowej (KDE), która pozwala na próbkowanie z gładkiego rozkładu odzwierciedlającego niepewność.\n",
    "3.  **Generowanie symulacji stochastycznych:** Uruchamiamy model SIRD wielokrotnie (tysiące razy), za każdym razem używając innego zestawu parametrów (wygenerowanego np. przez KDE lub poprzez dodanie szumu do najlepszych parametrów). Tworzy to \"chmurę\" możliwych przyszłych trajektorii epidemii, odzwierciedlającą skumulowaną niepewność parametrów i dynamiki modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec70ef",
   "metadata": {},
   "source": [
    "### 4.1 Bootstrapping parametrów modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_fit_parameters(df, start_date, end_date, n_bootstrap=5, bootstrap_fraction=0.8, use_enhanced_simulation=True):\n",
    "    \"\"\"Wykonuje bootstrapping parametrów. (Poprawiono formatowanie)\"\"\"\n",
    "    print(f\"Rozpoczynam bootstrapping parametrów (n={n_bootstrap})...\")\n",
    "    start_time = time.time()\n",
    "    df_train = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy().sort_values(\"Last_Update\").reset_index(drop=True)\n",
    "    if len(df_train) < 14:\n",
    "        print(\"Za mało danych dla bootstrappingu!\")\n",
    "        return []\n",
    "    bootstrap_params = []\n",
    "    for i in range(n_bootstrap):\n",
    "        print(f\"Bootstrap próbka {i+1}/{n_bootstrap}\")\n",
    "        df_pert = df_train.copy()\n",
    "        noise_a = np.mean(df_pert[\"Active\"]) * 0.05\n",
    "        noise_r = np.mean(df_pert[\"Recovered\"]) * 0.05\n",
    "        noise_d = np.mean(df_pert[\"Deaths\"]) * 0.05\n",
    "        df_pert[\"Active\"] += np.random.normal(0, noise_a, len(df_pert))\n",
    "        df_pert[\"Recovered\"] += np.random.normal(0, noise_r, len(df_pert))\n",
    "        df_pert[\"Deaths\"] += np.random.normal(0, noise_d, len(df_pert))\n",
    "        df_pert[\"Active\"] = df_pert[\"Active\"].clip(lower=1)\n",
    "        df_pert[\"Recovered\"] = df_pert[\"Recovered\"].clip(lower=1)\n",
    "        df_pert[\"Deaths\"] = df_pert[\"Deaths\"].clip(lower=1)\n",
    "        for j in range(1, len(df_pert)):\n",
    "            df_pert.loc[j, \"Recovered\"] = max(df_pert.loc[j, \"Recovered\"], df_pert.loc[j-1, \"Recovered\"])\n",
    "            df_pert.loc[j, \"Deaths\"] = max(df_pert.loc[j, \"Deaths\"], df_pert.loc[j-1, \"Deaths\"])\n",
    "        s_start = df_pert[\"Last_Update\"].min()\n",
    "        s_end = df_pert[\"Last_Update\"].max()\n",
    "        print(f\"  Próbka: {s_start:%Y-%m-%d} do {s_end:%Y-%m-%d} ({len(df_pert)} dni)\")\n",
    "        try:\n",
    "            _, params_i = fit_and_forecast(df_pert, s_start, s_end, 0, 1, n_particles=5000, max_iter=100, use_trend_correction=False)\n",
    "            if params_i:\n",
    "                bootstrap_params.append(params_i)\n",
    "            else:\n",
    "                print(f\"  Ostrzeżenie: Próbka {i+1} nie zwróciła parametrów.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Błąd dopasowania próbki {i+1}: {e}\")\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Bootstrapping zakończony w {elapsed:.1f}s. Uzyskano {len(bootstrap_params)}/{n_bootstrap} zestawów.\")\n",
    "    if bootstrap_params:\n",
    "        print(\"\\nStatystyki parametrów bootstrappowych:\")\n",
    "        valid_params = [p for p in bootstrap_params if p]\n",
    "        if valid_params:\n",
    "            param_names = list(valid_params[0].keys())\n",
    "            for param in param_names:\n",
    "                values = [p[param] for p in valid_params]\n",
    "                if values:\n",
    "                    mean = np.mean(values)\n",
    "                    std = np.std(values)\n",
    "                    min_v = np.min(values)\n",
    "                    max_v = np.max(values)\n",
    "                    print(f\"  {param}: mean={mean:.5f}, std={std:.5f}, min={min_v:.5f}, max={max_v:.5f}\")\n",
    "                else:\n",
    "                    print(f\"  {param}: Brak wartości.\")\n",
    "        else:\n",
    "            print(\"  Brak poprawnych parametrów do wyświetlenia statystyk.\")\n",
    "    return bootstrap_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8d512",
   "metadata": {},
   "source": [
    "### 4.2 Generowanie zróznicownaych parametrów (KDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diverse_parameters_with_kde(bootstrap_params, n_samples=1000):\n",
    "    print(f\"Generowanie {n_samples} zróżnicowanych zestawów parametrów z KDE...\")\n",
    "    valid_params = [p for p in bootstrap_params if p]\n",
    "    if len(valid_params) < 5:\n",
    "        print(\"  Za mało próbek dla KDE. Używam prostego próbkowania.\")\n",
    "        return _generate_diverse_parameters_simple(valid_params, n_samples)\n",
    "    try:\n",
    "        param_names = list(valid_params[0].keys())\n",
    "        param_values = {p: np.array([par[p] for par in valid_params]) for p in param_names}\n",
    "        param_bounds = {}\n",
    "        for p in param_names:\n",
    "            v = param_values[p]\n",
    "            mn = np.min(v)\n",
    "            mx = np.max(v)\n",
    "            rng = mx - mn\n",
    "            param_bounds[p] = (max(0, mn - 0.2 * rng), mx + 0.2 * rng)\n",
    "        kde_models = {p: stats.gaussian_kde(param_values[p]) for p in param_names}\n",
    "        diverse_params = []\n",
    "        gen_count = 0\n",
    "        attempts = 0\n",
    "        max_attempts = n_samples * 10\n",
    "        while gen_count < n_samples and attempts < max_attempts:\n",
    "            new_p = {}\n",
    "            valid = True\n",
    "            for p in param_names:\n",
    "                p_att = 0\n",
    "                max_p_att = 100\n",
    "                while p_att < max_p_att:\n",
    "                    val = kde_models[p].resample(1)[0][0] if kde_models[p].n > 0 else np.mean(param_values[p])\n",
    "                    if param_bounds[p][0] <= val <= param_bounds[p][1]:\n",
    "                        new_p[p] = val\n",
    "                        break\n",
    "                    p_att += 1\n",
    "                if p_att == max_p_att:\n",
    "                    valid = False\n",
    "                    break\n",
    "            if not valid:\n",
    "                attempts += 1\n",
    "                continue\n",
    "            if 't1' in new_p and 't2' in new_p and new_p['t2'] <= new_p['t1']:\n",
    "                new_p['t2'] = new_p['t1'] + np.random.uniform(1.0, 5.0)\n",
    "                new_p['t2'] = min(new_p['t2'], param_bounds.get('t2', (0, 40))[1])\n",
    "            diverse_params.append(new_p)\n",
    "            gen_count += 1\n",
    "            attempts += 1\n",
    "        if gen_count < n_samples:\n",
    "            print(f\"Ostrzeżenie: Wygenerowano tylko {gen_count}/{n_samples} zestawów z KDE.\")\n",
    "        print(f\"  Parametry wygenerowane z KDE ({gen_count} zestawów).\")\n",
    "        return diverse_params\n",
    "    except Exception as e:\n",
    "        print(f\"  Błąd KDE: {e}. Używam prostego próbkowania.\")\n",
    "        return _generate_diverse_parameters_simple(valid_params, n_samples)\n",
    "\n",
    "def _generate_diverse_parameters_simple(bootstrap_params, n_samples):\n",
    "    valid_params = [p for p in bootstrap_params if p]\n",
    "    if not valid_params:\n",
    "        print(\"Brak poprawnych parametrów bootstrap.\")\n",
    "        return []\n",
    "    param_names = list(valid_params[0].keys())\n",
    "    param_stats = {}\n",
    "    for p in param_names:\n",
    "        v = np.array([par[p] for par in valid_params])\n",
    "        param_stats[p] = {'mean': np.mean(v), 'std': np.std(v), 'min': np.min(v), 'max': np.max(v)}\n",
    "    param_bounds = {'beta1': (0.01, 1.5), 'beta2': (0.01, 1.5), 't1': (0.0, 15.0), 't2': (10.0, 40.0), 'gamma': (0.01, 0.3), 'mu': (0.001, 0.05)}\n",
    "    diverse_params = []\n",
    "    for _ in range(n_samples):\n",
    "        new_p = {}\n",
    "        for p in param_names:\n",
    "            mean = param_stats[p]['mean']\n",
    "            std = max(param_stats[p]['std'], mean * 0.1, 1e-6)\n",
    "            if p in param_bounds:\n",
    "                low, upp = param_bounds[p]\n",
    "                val = max(low, min(upp, np.random.normal(mean, std)))\n",
    "            else:\n",
    "                low = param_stats[p]['min'] * 0.8\n",
    "                upp = param_stats[p]['max'] * 1.2\n",
    "                val = max(low, min(upp, np.random.normal(mean, std)))\n",
    "            new_p[p] = val\n",
    "        if 't1' in new_p and 't2' in new_p and new_p['t2'] <= new_p['t1']:\n",
    "            new_p['t2'] = new_p['t1'] + np.random.uniform(1.0, 5.0)\n",
    "            new_p['t2'] = min(new_p['t2'], param_bounds.get('t2', (0, 40))[1])\n",
    "        diverse_params.append(new_p)\n",
    "    return diverse_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd47af4",
   "metadata": {},
   "source": [
    "### 4.3 Generowanie wielu symulacji stochastycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_improved_simulations(best_params, df, start_date, end_date, forecast_days=21, n_simulations=10000):\n",
    "    print(f\"\\nGenerowanie {n_simulations} ulepszonych symulacji stochastycznych...\")\n",
    "    dfw = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    days_window = len(dfw)\n",
    "    row0 = dfw.iloc[0]\n",
    "    S0 = POPULATION - (row0[\"Active\"] + row0[\"Recovered\"] + row0[\"Deaths\"])\n",
    "    I0, R0, D0 = row0[\"Active\"], row0[\"Recovered\"], row0[\"Deaths\"]\n",
    "    I_emp, R_emp, D_emp = dfw[\"Active\"].values, dfw[\"Recovered\"].values, dfw[\"Deaths\"].values\n",
    "    last_w = min(7, len(I_emp))\n",
    "    I_tr, R_tr, D_tr = (np.mean(np.diff(d[-last_w:]))/(np.mean(d[-last_w:])+1e-6) if last_w > 0 else 0 for d in [I_emp, R_emp, D_emp])\n",
    "    S_sims, I_sims, R_sims, D_sims = [], [], [], []\n",
    "    mod_vars = [(0.6, 1.0, 1.0, 1.0), (0.2, 0.9, 1.0, 1.0), (0.1, 1.1, 1.0, 1.0), (0.1, 1.0, 1.1, 0.9)]\n",
    "    if I_tr < -0.01:\n",
    "        mod_vars[1] = (0.3, 0.85, 1.0, 1.0)\n",
    "        mod_vars[2] = (0.05, 1.05, 1.0, 1.0)\n",
    "    elif I_tr > 0.01:\n",
    "        mod_vars[1] = (0.1, 0.95, 1.0, 1.0)\n",
    "        mod_vars[2] = (0.2, 1.15, 1.0, 1.0)\n",
    "    tot_w = sum(w for w,_,_,_ in mod_vars)\n",
    "    mod_vars = [(w/tot_w, mb, mg, mm) for w,mb,mg,mm in mod_vars]\n",
    "    sim_counts = [int(w * n_simulations) for w,_,_,_ in mod_vars]\n",
    "    diff = n_simulations - sum(sim_counts)\n",
    "    sim_counts[0] += diff\n",
    "    sim_tot = 0\n",
    "    for i, (w, b_mod, g_mod, m_mod) in enumerate(mod_vars):\n",
    "        n_sims = sim_counts[i]\n",
    "        print(f\"Model {i+1}: {n_sims} symulacji (waga {w:.2f})\")\n",
    "        for sim_idx in range(n_sims):\n",
    "            if sim_idx % 1000 == 0 and sim_idx > 0:\n",
    "                print(f\"  Symulacja {sim_idx}/{n_sims}\")\n",
    "            p = best_params.copy()\n",
    "            p[\"beta1\"] *= b_mod * np.random.normal(1.0, 0.05)\n",
    "            p[\"beta2\"] *= b_mod * np.random.normal(1.0, 0.05)\n",
    "            p[\"gamma\"] *= g_mod * np.random.normal(1.0, 0.05)\n",
    "            p[\"mu\"] *= m_mod * np.random.normal(1.0, 0.05)\n",
    "            p[\"beta1\"] = max(0.01, min(1.5, p[\"beta1\"]))\n",
    "            p[\"beta2\"] = max(0.01, min(1.5, p[\"beta2\"]))\n",
    "            p[\"gamma\"] = max(0.01, min(0.3, p[\"gamma\"]))\n",
    "            p[\"mu\"] = max(0.001, min(0.05, p[\"mu\"]))\n",
    "            p[\"t1\"] += np.random.normal(0, 0.5)\n",
    "            p[\"t2\"] += np.random.normal(0, 0.5)\n",
    "            p[\"t1\"] = max(0, min(10, p[\"t1\"]))\n",
    "            p[\"t2\"] = max(p[\"t1\"] + 1, min(36, p[\"t2\"]))\n",
    "            S_f, I_f, R_f, D_f = enhanced_simulate_sird_rk4(p, days_window, S0, I0, R0, D0, start_date=start_date)\n",
    "            if forecast_days > 0:\n",
    "                sl, il, rl, dl = S_f[-1], I_f[-1], R_f[-1], D_f[-1]\n",
    "                damp = 0.97 if i == 0 else 0.95 if i == 1 else 0.98\n",
    "                fc_p = p.copy()\n",
    "                fc_p[\"beta1\"] = p[\"beta2\"] * damp\n",
    "                fc_p[\"beta2\"] = p[\"beta2\"] * (damp ** 2)\n",
    "                fc_p[\"t1\"] = 0\n",
    "                fc_p[\"t2\"] = min(forecast_days * 0.4, 7)\n",
    "                S_fc, I_fc, R_fc, D_fc = enhanced_simulate_sird_rk4(fc_p, forecast_days, sl, il, rl, dl, forecast_start_day=0, forecast_feedback=True, start_date=end_date + pd.Timedelta(days=1))\n",
    "                S_full = np.concatenate([S_f, S_fc])\n",
    "                I_full = np.concatenate([I_f, I_fc])\n",
    "                R_full = np.concatenate([R_f, R_fc])\n",
    "                D_full = np.concatenate([D_f, D_fc])\n",
    "            else:\n",
    "                S_full, I_full, R_full, D_full = S_f, I_f, R_f, D_f\n",
    "            S_sims.append(S_full)\n",
    "            I_sims.append(I_full)\n",
    "            R_sims.append(R_full)\n",
    "            D_sims.append(D_full)\n",
    "            sim_tot += 1\n",
    "    def trim_outliers(sims, perc=2.0):\n",
    "        if not sims:\n",
    "            return []\n",
    "        sim_arr = np.array(sims)\n",
    "        if sim_arr.size == 0:\n",
    "            return []\n",
    "        for t in range(sim_arr.shape[1]):\n",
    "            vals = sim_arr[:, t]\n",
    "            low = np.percentile(vals, perc)\n",
    "            upp = np.percentile(vals, 100 - perc)\n",
    "            out = (vals < low) | (vals > upp)\n",
    "            if np.any(out):\n",
    "                valid = vals[~out]\n",
    "                if valid.size > 0:\n",
    "                    replacements = np.random.choice(valid, size=np.sum(out))\n",
    "                    sim_arr[out, t] = replacements\n",
    "                else:\n",
    "                    sim_arr[out, t] = np.median(vals)\n",
    "        return [sim_arr[i, :] for i in range(sim_arr.shape[0])]\n",
    "    print(\"Przycinanie odstających wartości...\")\n",
    "    S_sims = trim_outliers(S_sims)\n",
    "    I_sims = trim_outliers(I_sims)\n",
    "    R_sims = trim_outliers(R_sims)\n",
    "    D_sims = trim_outliers(D_sims)\n",
    "    return S_sims, I_sims, R_sims, D_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a20a2",
   "metadata": {},
   "source": [
    "## 5. Tworzenie zespołu prognoz i przedziałów ufności"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28228c",
   "metadata": {},
   "source": [
    "### 5.1 Filtrowanie trajektorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38228d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trajectories(S_sims, I_sims, R_sims, D_sims, df, start_date, end_date, percentile=70):\n",
    "    dfw = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    I_emp = dfw[\"Active\"].values\n",
    "    R_emp = dfw[\"Recovered\"].values\n",
    "    D_emp = dfw[\"Deaths\"].values\n",
    "    n_days = len(dfw)\n",
    "    day_w = np.linspace(1.0, 3.0, n_days)\n",
    "    errors = []\n",
    "    for i in range(len(I_sims)):\n",
    "        if len(I_sims[i]) >= n_days:\n",
    "            i_err = np.average((I_sims[i][:n_days] - I_emp)**2, weights=day_w)\n",
    "            r_err = np.average((R_sims[i][:n_days] - R_emp)**2, weights=day_w)\n",
    "            d_err = np.average((D_sims[i][:n_days] - D_emp)**2, weights=day_w)\n",
    "            errors.append(i_err*2.5 + r_err*0.8 + d_err*3.0)\n",
    "        else:\n",
    "            errors.append(float('inf'))\n",
    "    if not errors:\n",
    "        print(\"Brak trajektorii do filtrowania.\")\n",
    "        return S_sims, I_sims, R_sims, D_sims\n",
    "    valid_err = [e for e in errors if e != float('inf')]\n",
    "    if not valid_err:\n",
    "        print(\"Brak poprawnych trajektorii do filtrowania.\")\n",
    "        return [], [], [], []\n",
    "    threshold = np.percentile(valid_err, percentile)\n",
    "    good_idx = [i for i, err in enumerate(errors) if err <= threshold]\n",
    "    if not good_idx:\n",
    "        print(f\"Żadna trajektoria nie przeszła filtrowania (percentyl {percentile}). Zwracam wszystkie poprawne.\")\n",
    "        good_idx = [i for i, err in enumerate(errors) if err != float('inf')]\n",
    "    if not good_idx:\n",
    "        print(\"Brak jakichkolwiek poprawnych trajektorii.\")\n",
    "        return [], [], [], []\n",
    "    print(f\"Filtrowanie: Pozostawiono {len(good_idx)}/{len(I_sims)} (percentyl {percentile})\")\n",
    "    S_filtered = [S_sims[i] for i in good_idx]\n",
    "    I_filtered = [I_sims[i] for i in good_idx]\n",
    "    R_filtered = [R_sims[i] for i in good_idx]\n",
    "    D_filtered = [D_sims[i] for i in good_idx]\n",
    "    return S_filtered, I_filtered, R_filtered, D_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4988e",
   "metadata": {},
   "source": [
    "### 5.2 Tworzenie przedziałów ufności (standardowe i adaptacyjne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_ensemble_with_wider_intervals(S_sims, I_sims, R_sims, D_sims,\n",
    "                           df, start_date, end_date, ensemble_size=100,\n",
    "                           confidence_levels=[50, 80, 95], width_factor=1.5, diversify=True):\n",
    "    dfw = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    I_emp, R_emp, D_emp = dfw[\"Active\"].values, dfw[\"Recovered\"].values, dfw[\"Deaths\"].values\n",
    "    S_array, I_array, R_array, D_array = map(np.array, [S_sims, I_sims, R_sims, D_sims])\n",
    "    train_days = len(I_emp)\n",
    "    errors = []\n",
    "    for i in range(len(I_sims)):\n",
    "         if len(I_array[i]) >= train_days:\n",
    "             i_err = np.mean(np.abs(I_array[i, :train_days] - I_emp))\n",
    "             r_err = np.mean(np.abs(R_array[i, :train_days] - R_emp))\n",
    "             d_err = np.mean(np.abs(D_array[i, :train_days] - D_emp))\n",
    "             errors.append(i_err * 2.0 + r_err + d_err * 5.0)\n",
    "         else:\n",
    "             errors.append(float('inf'))\n",
    "    errors = np.array(errors)\n",
    "    valid_indices = np.where(errors != float('inf'))[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"Brak poprawnych symulacji dla ensemble.\")\n",
    "        return {'S': {}, 'I': {}, 'R': {}, 'D': {}}\n",
    "    errors_valid = errors[valid_indices]\n",
    "\n",
    "    if diversify and len(valid_indices) >= 10:\n",
    "        sample_rate = max(1, train_days // 10)\n",
    "        features = I_array[valid_indices][:, :train_days:sample_rate]\n",
    "        n_clusters = min(10, ensemble_size // 5, len(valid_indices) // 2)\n",
    "        if n_clusters < 2:\n",
    "            print(\"Za mało danych do klastrowania, używam prostej metody.\")\n",
    "            diversify = False\n",
    "        else:\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                clusters = kmeans.fit_predict(features)\n",
    "                selected = []\n",
    "                for cid in range(n_clusters):\n",
    "                    c_idx = np.where(clusters == cid)[0]\n",
    "                    c_idx_orig = valid_indices[c_idx]\n",
    "                    if len(c_idx) > 0:\n",
    "                        n_sel = max(1, int(ensemble_size * len(c_idx) / len(valid_indices)))\n",
    "                        c_errs = errors_valid[c_idx]\n",
    "                        best = c_idx_orig[np.argsort(c_errs)[:n_sel]]\n",
    "                        selected.extend(best)\n",
    "                if len(selected) < ensemble_size:\n",
    "                    n_rem = ensemble_size - len(selected)\n",
    "                    rem_idx = list(set(valid_indices) - set(selected))\n",
    "                    best_rem = np.argsort(errors[rem_idx])[:n_rem]\n",
    "                    selected.extend([rem_idx[i] for i in best_rem])\n",
    "                top_indices = np.array(selected[:ensemble_size])\n",
    "            except Exception as e:\n",
    "                print(f\"Błąd klastrowania: {e}, używam prostej metody.\")\n",
    "                top_indices = valid_indices[np.argsort(errors_valid)[:ensemble_size]]\n",
    "    else:\n",
    "        top_indices = valid_indices[np.argsort(errors_valid)[:ensemble_size]]\n",
    "\n",
    "    S_ens, I_ens, R_ens, D_ens = S_array[top_indices], I_array[top_indices], R_array[top_indices], D_array[top_indices]\n",
    "    train_err_i = np.mean(np.abs(I_ens[:, :train_days] - I_emp.reshape(1, -1)))\n",
    "    train_err_r = np.mean(np.abs(R_ens[:, :train_days] - R_emp.reshape(1, -1)))\n",
    "    train_err_d = np.mean(np.abs(D_ens[:, :train_days] - D_emp.reshape(1, -1)))\n",
    "    avg_err = (train_err_i/(np.mean(I_emp)+1e-6) + train_err_r/(np.mean(R_emp)+1e-6) + train_err_d/(np.mean(D_emp)+1e-6))/3\n",
    "    dyn_width = width_factor * (1.0 + min(1.0, avg_err))\n",
    "    print(f\"Dynamiczne szerokości: {dyn_width:.4f}\")\n",
    "    results = {'S': {'median': np.median(S_ens, axis=0)}, 'I': {'median': np.median(I_ens, axis=0)}, 'R': {'median': np.median(R_ens, axis=0)}, 'D': {'median': np.median(D_ens, axis=0)}}\n",
    "    total_len = S_ens.shape[1]\n",
    "\n",
    "    for level in confidence_levels:\n",
    "        lp, up = (100 - level) / 2, 100 - (100 - level) / 2\n",
    "        adj_lp = max(0, lp - (dyn_width - 1) * lp)\n",
    "        adj_up = min(100, up + (dyn_width - 1) * (100 - up))\n",
    "        results['S'][f'{level}'] = {'lower': np.percentile(S_ens, adj_lp, axis=0), 'upper': np.percentile(S_ens, adj_up, axis=0)}\n",
    "        results['I'][f'{level}'] = {'lower': np.percentile(I_ens, adj_lp, axis=0), 'upper': np.percentile(I_ens, adj_up, axis=0)}\n",
    "        results['R'][f'{level}'] = {'lower': np.percentile(R_ens, adj_lp, axis=0), 'upper': np.percentile(R_ens, adj_up, axis=0)}\n",
    "        results['D'][f'{level}'] = {'lower': np.percentile(D_ens, adj_lp, axis=0), 'upper': np.percentile(D_ens, adj_up, axis=0)}\n",
    "\n",
    "    try:\n",
    "        for level in confidence_levels:\n",
    "            for comp in ['S', 'I', 'R', 'D']:\n",
    "                sigma = 1.0\n",
    "                results[comp][f'{level}']['lower'] = gaussian_filter1d(results[comp][f'{level}']['lower'], sigma)\n",
    "                results[comp][f'{level}']['upper'] = gaussian_filter1d(results[comp][f'{level}']['upper'], sigma)\n",
    "                if train_days < total_len:\n",
    "                    fc_lower = results[comp][f'{level}']['lower'][train_days:].copy()\n",
    "                    fc_upper = results[comp][f'{level}']['upper'][train_days:].copy()\n",
    "                    fc_sigma = sigma * 1.5\n",
    "                    results[comp][f'{level}']['lower'][train_days:] = gaussian_filter1d(fc_lower, fc_sigma)\n",
    "                    results[comp][f'{level}']['upper'][train_days:] = gaussian_filter1d(fc_upper, fc_sigma)\n",
    "    except ImportError:\n",
    "        print(\"Scipy not found, skipping CI smoothing.\")\n",
    "\n",
    "    if train_days < total_len:\n",
    "        for level in confidence_levels:\n",
    "            for comp in ['S', 'I', 'R', 'D']:\n",
    "                for t in range(train_days, total_len):\n",
    "                    fc_day = t - train_days\n",
    "                    fc_factor = 1.0 + (dyn_width - 1.0) * (1.0 + 0.03 * fc_day + 0.003 * fc_day**2)\n",
    "                    mid = (results[comp][f'{level}']['upper'][t] + results[comp][f'{level}']['lower'][t]) / 2\n",
    "                    width = results[comp][f'{level}']['upper'][t] - results[comp][f'{level}']['lower'][t]\n",
    "                    results[comp][f'{level}']['lower'][t] = mid - fc_factor * width / 2\n",
    "                    results[comp][f'{level}']['upper'][t] = mid + fc_factor * width / 2\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adaptive_confidence_intervals(S_sims, I_sims, R_sims, D_sims, df, start_date, end_date, ensemble_size=350, confidence_levels=[50, 80, 95]):\n",
    "    dfw = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    I_emp, R_emp, D_emp = dfw[\"Active\"].values, dfw[\"Recovered\"].values, dfw[\"Deaths\"].values\n",
    "    S_array, I_array, R_array, D_array = map(np.array, [S_sims, I_sims, R_sims, D_sims])\n",
    "    train_days = len(I_emp)\n",
    "    errors_I, errors_R, errors_D, errors_S = [], [], [], []\n",
    "    for i in range(len(I_sims)):\n",
    "        if len(I_array[i]) >= train_days:\n",
    "            i_err = np.mean(np.abs(I_array[i, :train_days] - I_emp) / (I_emp + 1))\n",
    "            r_err = np.mean(np.abs(R_array[i, :train_days] - R_emp) / (R_emp + 1))\n",
    "            d_err = np.mean(np.abs(D_array[i, :train_days] - D_emp) / (D_emp + 1))\n",
    "            s_err = np.std(S_array[i, :train_days]) / (np.mean(S_array[i, :train_days]) + 1e-6)\n",
    "            errors_I.append(i_err)\n",
    "            errors_R.append(r_err)\n",
    "            errors_D.append(d_err)\n",
    "            errors_S.append(s_err)\n",
    "        else:\n",
    "            errors_I.append(float('inf'))\n",
    "            errors_R.append(float('inf'))\n",
    "            errors_D.append(float('inf'))\n",
    "            errors_S.append(float('inf'))\n",
    "    valid_idx = [i for i, err in enumerate(errors_I) if err != float('inf')]\n",
    "    if not valid_idx:\n",
    "        print(\"Brak poprawnych symulacji dla adaptacyjnych CI.\")\n",
    "        return {'S': {}, 'I': {}, 'R': {}, 'D': {}}\n",
    "    err_I_v, err_R_v, err_D_v, err_S_v = (np.array([errors[i] for i in valid_idx]) for errors in [errors_I, errors_R, errors_D, errors_S])\n",
    "    valid_idx_arr = np.array(valid_idx)\n",
    "    top_I = valid_idx_arr[np.argsort(err_I_v)[:ensemble_size]]\n",
    "    top_R = valid_idx_arr[np.argsort(err_R_v)[:ensemble_size]]\n",
    "    top_D = valid_idx_arr[np.argsort(err_D_v)[:ensemble_size]]\n",
    "    top_S = valid_idx_arr[np.argsort(err_S_v)[:ensemble_size]]\n",
    "    I_ens, R_ens, D_ens, S_ens = I_array[top_I], R_array[top_R], D_array[top_D], S_array[top_S]\n",
    "    width_f = {'S': 1.1, 'I': 1.3, 'R': 1.2, 'D': 1.25}\n",
    "    train_err_i = np.mean(np.abs(I_ens[:, :train_days] - I_emp.reshape(1, -1)) / (I_emp.reshape(1, -1) + 1))\n",
    "    train_err_r = np.mean(np.abs(R_ens[:, :train_days] - R_emp.reshape(1, -1)) / (R_emp.reshape(1, -1) + 1))\n",
    "    train_err_d = np.mean(np.abs(D_ens[:, :train_days] - D_emp.reshape(1, -1)) / (D_emp.reshape(1, -1) + 1))\n",
    "    dyn_w_I = width_f['I'] * (1.0 + min(1.0, train_err_i * 3))\n",
    "    dyn_w_R = width_f['R'] * (1.0 + min(1.0, train_err_r * 3))\n",
    "    dyn_w_D = width_f['D'] * (1.0 + min(1.0, train_err_d * 3))\n",
    "    dyn_w_S = width_f['S']\n",
    "    print(f\"Dynamiczne szerokości: S={dyn_w_S:.4f}, I={dyn_w_I:.4f}, R={dyn_w_R:.4f}, D={dyn_w_D:.4f}\")\n",
    "    results = {'S': {'median': np.median(S_ens, axis=0)}, 'I': {'median': np.median(I_ens, axis=0)}, 'R': {'median': np.median(R_ens, axis=0)}, 'D': {'median': np.median(D_ens, axis=0)}}\n",
    "    total_len = S_ens.shape[1]\n",
    "    for level in confidence_levels:\n",
    "        lp, up = (100 - level) / 2, 100 - (100 - level) / 2\n",
    "        adj_lp_S = max(0, lp - (dyn_w_S - 1) * lp)\n",
    "        adj_up_S = min(100, up + (dyn_w_S - 1) * (100 - up))\n",
    "        adj_lp_I = max(0, lp - (dyn_w_I - 1) * lp)\n",
    "        adj_up_I = min(100, up + (dyn_w_I - 1) * (100 - up))\n",
    "        adj_lp_R = max(0, lp - (dyn_w_R - 1) * lp)\n",
    "        adj_up_R = min(100, up + (dyn_w_R - 1) * (100 - up))\n",
    "        adj_lp_D = max(0, lp - (dyn_w_D - 1) * lp)\n",
    "        adj_up_D = min(100, up + (dyn_w_D - 1) * (100 - up))\n",
    "        results['S'][f'{level}'] = {'lower': np.percentile(S_ens, adj_lp_S, axis=0), 'upper': np.percentile(S_ens, adj_up_S, axis=0)}\n",
    "        results['I'][f'{level}'] = {'lower': np.percentile(I_ens, adj_lp_I, axis=0), 'upper': np.percentile(I_ens, adj_up_I, axis=0)}\n",
    "        results['R'][f'{level}'] = {'lower': np.percentile(R_ens, adj_lp_R, axis=0), 'upper': np.percentile(R_ens, adj_up_R, axis=0)}\n",
    "        results['D'][f'{level}'] = {'lower': np.percentile(D_ens, adj_lp_D, axis=0), 'upper': np.percentile(D_ens, adj_up_D, axis=0)}\n",
    "    smooth_p = {'S': 0.8, 'I': 1.2, 'R': 1.0, 'D': 0.9}\n",
    "    for level in confidence_levels:\n",
    "        for comp in ['S', 'I', 'R', 'D']:\n",
    "            sigma = smooth_p[comp]\n",
    "            results[comp][f'{level}']['lower'] = gaussian_filter1d(results[comp][f'{level}']['lower'], sigma)\n",
    "            results[comp][f'{level}']['upper'] = gaussian_filter1d(results[comp][f'{level}']['upper'], sigma)\n",
    "            if train_days < total_len:\n",
    "                fc_l = results[comp][f'{level}']['lower'][train_days:].copy()\n",
    "                fc_u = results[comp][f'{level}']['upper'][train_days:].copy()\n",
    "                fc_s = sigma * 1.5\n",
    "                results[comp][f'{level}']['lower'][train_days:] = gaussian_filter1d(fc_l, fc_s)\n",
    "                results[comp][f'{level}']['upper'][train_days:] = gaussian_filter1d(fc_u, fc_s)\n",
    "    fc_growth = {'S': 0.015, 'I': 0.07, 'R': 0.035, 'D': 0.02}\n",
    "    if train_days < total_len:\n",
    "        for level in confidence_levels:\n",
    "            for comp in ['S', 'I', 'R', 'D']:\n",
    "                gf = fc_growth[comp]\n",
    "                wf = dyn_w_S if comp == 'S' else dyn_w_I if comp == 'I' else dyn_w_R if comp == 'R' else dyn_w_D\n",
    "                for t in range(train_days, total_len):\n",
    "                    fc_day = t - train_days\n",
    "                    fc_factor = 1.0 + (wf - 1.0) * (1.0 + gf * fc_day + 0.05 * fc_day**2)\n",
    "                    mid = (results[comp][f'{level}']['upper'][t] + results[comp][f'{level}']['lower'][t]) / 2\n",
    "                    width = results[comp][f'{level}']['upper'][t] - results[comp][f'{level}']['lower'][t]\n",
    "                    results[comp][f'{level}']['lower'][t] = mid - fc_factor * width / 2\n",
    "                    results[comp][f'{level}']['upper'][t] = mid + fc_factor * width / 2\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce32d5",
   "metadata": {},
   "source": [
    "### 5.3 Korekty, ograniczenia dla przedziałów ufności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_data_driven_boundary_correction(results, df, start_date, end_date, forecast_days):\n",
    "    df_train = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy().reset_index(drop=True)\n",
    "    I_emp, R_emp, D_emp = df_train[\"Active\"].values, df_train[\"Recovered\"].values, df_train[\"Deaths\"].values\n",
    "    def analyze_rate(data):\n",
    "        if len(data) <= 1: return {\"mean\": 0, \"std\": 0, \"max\": 0, \"min\": 0}\n",
    "        ch = [(data[i] - data[i-1]) / data[i-1] for i in range(1, len(data)) if data[i-1] > 1e-9]\n",
    "        if not ch: return {\"mean\": 0, \"std\": 0, \"max\": 0, \"min\": 0}\n",
    "        return {\"mean\": np.mean(ch), \"std\": np.std(ch), \"max\": np.max(ch), \"min\": np.min(ch)}\n",
    "    I_ch, R_ch, D_ch = analyze_rate(I_emp), analyze_rate(R_emp), analyze_rate(D_emp)\n",
    "    train_days = len(df_train)\n",
    "    if 'S' not in results or 'median' not in results['S'] or results['S']['median'].size == 0:\n",
    "        print(\"Brak danych 'S' do korekcji granic.\")\n",
    "        return results\n",
    "    total_days = len(results['S']['median'])\n",
    "    corr_res = {comp: {'median': results[comp]['median'].copy()} for comp in ['S', 'I', 'R', 'D']}\n",
    "    conf_levels = [k for k in results['S'].keys() if k != 'median']\n",
    "    for level in conf_levels:\n",
    "        for comp in ['S', 'I', 'R', 'D']:\n",
    "            if level in results[comp] and 'lower' in results[comp][level]:\n",
    "                corr_res[comp][level] = {'lower': results[comp][level]['lower'].copy(), 'upper': results[comp][level]['upper'].copy()}\n",
    "            else:\n",
    "                print(f\"Brak danych {level}/{comp} w korekcji granic.\")\n",
    "                corr_res[comp][level] = {'lower': corr_res[comp]['median'].copy(), 'upper': corr_res[comp]['median'].copy()}\n",
    "    if forecast_days > 0 and total_days > train_days:\n",
    "        i_max_r, i_min_r = min(I_ch[\"max\"] * 1.5, 0.25), max(I_ch[\"min\"] * 1.5, -0.2)\n",
    "        r_max_r, r_min_r = min(R_ch[\"max\"] * 1.3, 0.2), max(R_ch[\"min\"], -0.05)\n",
    "        d_max_r, d_min_r = min(D_ch[\"max\"] * 1.3, 0.1), 0\n",
    "        s_max_r, s_min_r = 0.01, -0.01\n",
    "        for day in range(train_days, total_days):\n",
    "            prev = max(0, day - 1)\n",
    "            for level in conf_levels:\n",
    "                prev_il, prev_iu = corr_res['I'][level]['lower'][prev], corr_res['I'][level]['upper'][prev]\n",
    "                il_lim, iu_lim = prev_il * (1 + i_min_r), prev_iu * (1 + i_max_r)\n",
    "                corr_res['I'][level]['lower'][day] = max(corr_res['I'][level]['lower'][day], il_lim)\n",
    "                corr_res['I'][level]['upper'][day] = min(corr_res['I'][level]['upper'][day], iu_lim)\n",
    "\n",
    "                prev_rl, prev_ru = corr_res['R'][level]['lower'][prev], corr_res['R'][level]['upper'][prev]\n",
    "                rl_lim, ru_lim = prev_rl * (1 + r_min_r), prev_ru * (1 + r_max_r)\n",
    "                corr_res['R'][level]['lower'][day] = max(corr_res['R'][level]['lower'][day], rl_lim)\n",
    "                corr_res['R'][level]['upper'][day] = min(corr_res['R'][level]['upper'][day], ru_lim)\n",
    "\n",
    "                prev_dl, prev_du = corr_res['D'][level]['lower'][prev], corr_res['D'][level]['upper'][prev]\n",
    "                dl_lim, du_lim = prev_dl, prev_du * (1 + d_max_r)\n",
    "                corr_res['D'][level]['lower'][day] = max(corr_res['D'][level]['lower'][day], dl_lim)\n",
    "                corr_res['D'][level]['upper'][day] = min(corr_res['D'][level]['upper'][day], du_lim)\n",
    "\n",
    "                prev_sl, prev_su = corr_res['S'][level]['lower'][prev], corr_res['S'][level]['upper'][prev]\n",
    "                sl_lim, su_lim = prev_sl * (1 + s_min_r), prev_su * (1 + s_max_r)\n",
    "                corr_res['S'][level]['lower'][day] = max(corr_res['S'][level]['lower'][day], sl_lim)\n",
    "                corr_res['S'][level]['upper'][day] = min(corr_res['S'][level]['upper'][day], su_lim)\n",
    "    if forecast_days > 0 and total_days > train_days:\n",
    "        for level in conf_levels:\n",
    "            for comp in ['S', 'I', 'R', 'D']:\n",
    "                 if level in corr_res[comp]:\n",
    "                    fc_l = corr_res[comp][level]['lower'][train_days:].copy()\n",
    "                    fc_u = corr_res[comp][level]['upper'][train_days:].copy()\n",
    "                    sigma = 1.0\n",
    "                    corr_res[comp][level]['lower'][train_days:] = gaussian_filter1d(fc_l, sigma)\n",
    "                    corr_res[comp][level]['upper'][train_days:] = gaussian_filter1d(fc_u, sigma)\n",
    "    return corr_res\n",
    "\n",
    "def enforce_compartment_constraints(results, population=POPULATION):\n",
    "    if 'S' not in results or 'median' not in results['S'] or results['S']['median'].size == 0:\n",
    "        print(\"Brak danych 'S' do nałożenia ograniczeń.\")\n",
    "        return results\n",
    "    corrected = {comp: {'median': results[comp]['median'].copy()} for comp in ['S', 'I', 'R', 'D']}\n",
    "    total_days = len(results['S']['median'])\n",
    "    for t in range(total_days):\n",
    "        tot = sum(results[comp]['median'][t] for comp in ['S', 'I', 'R', 'D'])\n",
    "        if abs(tot - population) > 1e-6:\n",
    "            scale = population / (tot + 1e-9)\n",
    "            for comp in ['S', 'I', 'R', 'D']:\n",
    "                corrected[comp]['median'][t] = results[comp]['median'][t] * scale\n",
    "\n",
    "    conf_levels = [k for k in results['S'].keys() if k != 'median']\n",
    "    for level in conf_levels:\n",
    "        for comp in ['S', 'I', 'R', 'D']:\n",
    "            if level in results[comp] and 'lower' in results[comp][level]:\n",
    "                corrected[comp][level] = {'lower': results[comp][level]['lower'].copy(), 'upper': results[comp][level]['upper'].copy()}\n",
    "            else:\n",
    "                print(f\"Brak danych {level}/{comp} w enforce_constraints.\")\n",
    "                corrected[comp][level] = {'lower': corrected[comp]['median'].copy(), 'upper': corrected[comp]['median'].copy()}\n",
    "    for level in conf_levels:\n",
    "        for comp in ['S', 'I', 'R', 'D']:\n",
    "            corrected[comp][level]['lower'] = np.maximum(corrected[comp][level]['lower'], 0)\n",
    "    for t in range(total_days):\n",
    "        for level in conf_levels:\n",
    "            low_sum = sum(corrected[comp][level]['lower'][t] for comp in ['S', 'I', 'R', 'D'])\n",
    "            upp_sum = sum(corrected[comp][level]['upper'][t] for comp in ['S', 'I', 'R', 'D'])\n",
    "            if low_sum > population:\n",
    "                scale = population / (low_sum + 1e-9)\n",
    "                for comp in ['S', 'I', 'R', 'D']:\n",
    "                    corrected[comp][level]['lower'][t] *= scale\n",
    "            if upp_sum < population and upp_sum > 1e-9:\n",
    "                scale = population / upp_sum\n",
    "                for comp in ['S', 'I', 'R', 'D']:\n",
    "                    corrected[comp][level]['upper'][t] *= scale\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b1a7a",
   "metadata": {},
   "source": [
    "## 6. Wizualizacja Wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541deda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stochastic_results(S_simulations, I_simulations, R_simulations, D_simulations, df, start_date, end_date, forecast_days=21, title=\"SIRD Model Forecast\", tick_step=7, confidence_levels=[50, 80, 95], ensemble_results=None, plot_path=None, figsize=(14, 20), legend_loc='upper left'):\n",
    "    limit_end_date = end_date + pd.Timedelta(days=forecast_days)\n",
    "    df_plot = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= limit_end_date)].copy().reset_index(drop=True)\n",
    "    if df_plot.empty:\n",
    "        print(\"Brak danych do wykresu.\")\n",
    "        return None\n",
    "    train_days = (end_date - start_date).days + 1\n",
    "    train_days = min(train_days, len(df_plot))\n",
    "    if not S_simulations:\n",
    "        print(\"Brak symulacji do wykresu.\")\n",
    "        return None\n",
    "    sim_len = len(S_simulations[0]) if S_simulations else 0\n",
    "    data_len = len(df_plot)\n",
    "    plot_len = max(sim_len, data_len)\n",
    "    plot_len = min(plot_len, train_days + forecast_days)\n",
    "    pop = POPULATION\n",
    "    S_emp = pop - (df_plot[\"Active\"] + df_plot[\"Recovered\"] + df_plot[\"Deaths\"]).values\n",
    "    I_emp = df_plot[\"Active\"].values\n",
    "    R_emp = df_plot[\"Recovered\"].values\n",
    "    D_emp = df_plot[\"Deaths\"].values\n",
    "    S_emp, I_emp, R_emp, D_emp = (d[:plot_len] for d in [S_emp, I_emp, R_emp, D_emp])\n",
    "    date_labels = df_plot[\"Last_Update\"].dt.strftime(\"%Y-%m-%d\").values[:plot_len]\n",
    "    x_data = np.arange(plot_len)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axs = plt.subplots(4, 1, figsize=figsize, sharex=True)\n",
    "    colors = {'S': '#0072B2', 'I': '#D55E00', 'R': '#009E73', 'D': '#CC79A7', 'S_forecast': '#56B4E9', 'I_forecast': '#E69F00', 'R_forecast': '#56E9BA', 'D_forecast': '#F0B6D4', 'data': '#000000', 'forecast_line': '#666666', 'median': '#000000'}\n",
    "    markers = {'S': 'o', 'I': 's', 'R': '^', 'D': 'd'}\n",
    "    S_arr, I_arr, R_arr, D_arr = (np.array([s[:plot_len] for s in sims]) for sims in [S_simulations, I_simulations, R_simulations, D_simulations])\n",
    "\n",
    "    if ensemble_results is not None and 'S' in ensemble_results and 'median' in ensemble_results['S']:\n",
    "        S_med, I_med, R_med, D_med = (ensemble_results[c]['median'][:plot_len] for c in ['S', 'I', 'R', 'D'])\n",
    "        conf_bounds = {}\n",
    "        for level in confidence_levels:\n",
    "            conf_bounds[level] = {c: {'lower': ensemble_results[c][f'{level}']['lower'][:plot_len], 'upper': ensemble_results[c][f'{level}']['upper'][:plot_len]} for c in ['S', 'I', 'R', 'D']}\n",
    "    else:\n",
    "        S_med, I_med, R_med, D_med = (np.median(arr, axis=0) for arr in [S_arr, I_arr, R_arr, D_arr])\n",
    "        conf_bounds = {}\n",
    "        for level in confidence_levels:\n",
    "            lp = (100 - level) / 2\n",
    "            up = 100 - lp\n",
    "            conf_bounds[level] = {c: {'lower': np.percentile(arr, lp, axis=0), 'upper': np.percentile(arr, up, axis=0)} for c, arr in zip(['S', 'I', 'R', 'D'], [S_arr, I_arr, R_arr, D_arr])}\n",
    "\n",
    "    train_range = x_data[:min(train_days, plot_len)]\n",
    "    fc_range = x_data[min(train_days, plot_len):plot_len] if forecast_days > 0 and plot_len > train_days else []\n",
    "    fc_start_idx = min(train_days, plot_len)\n",
    "    alpha_lvls = {95: 0.2, 80: 0.3, 50: 0.4}\n",
    "    subplot_info = [(axs[0], 'S', 'Susceptible (S)'), (axs[1], 'I', 'Infected/Active (I)'), (axs[2], 'R', 'Recovered (R)'), (axs[3], 'D', 'Deaths (D)')]\n",
    "\n",
    "    for ax, key, lbl in subplot_info:\n",
    "        for level in sorted(confidence_levels, reverse=True):\n",
    "            bnds = conf_bounds[level]\n",
    "            alpha = alpha_lvls.get(level, 0.3)\n",
    "            ax.fill_between(train_range, bnds[key]['lower'][:len(train_range)], bnds[key]['upper'][:len(train_range)], color=colors[key], alpha=alpha, label=f\"{level}% CI (training)\")\n",
    "            if len(fc_range) > 0:\n",
    "                ax.fill_between(fc_range, bnds[key]['lower'][fc_start_idx:plot_len], bnds[key]['upper'][fc_start_idx:plot_len], color=colors[f\"{key}_forecast\"], alpha=alpha, label=f\"{level}% CI (forecast)\")\n",
    "        ax.plot(train_range, eval(f\"{key}_med[:len(train_range)]\"), color=colors['median'], lw=2, ls='-', label=\"Median (training)\")\n",
    "        if len(fc_range) > 0:\n",
    "            ax.plot(fc_range, eval(f\"{key}_med[fc_start_idx:plot_len]\"), color=colors['median'], lw=2, ls='--', label=\"Median (forecast)\")\n",
    "        emp = eval(f\"{key}_emp\")\n",
    "        if key != 'S':\n",
    "            ax.scatter(x_data[:len(emp)], emp, color=colors['data'], s=50, marker=markers[key], edgecolor='white', lw=0.5, label=\"Empirical data\")\n",
    "        if forecast_days > 0 and plot_len > train_days:\n",
    "            ax.axvline(fc_start_idx - 1, color=colors['forecast_line'], ls='--', lw=1.5, label=\"Forecast start\")\n",
    "        ax.set_ylabel(\"Population\", fontsize=14, fontweight='bold')\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "        ax.set_title(lbl, fontsize=16, fontweight='bold')\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        handles, labels_list = ax.get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels_list, handles))\n",
    "        order = [\"Empirical data\", \"Median (training)\", \"Median (forecast)\", \"Forecast start\", \"50% CI (training)\", \"50% CI (forecast)\", \"80% CI (training)\", \"80% CI (forecast)\", \"95% CI (training)\", \"95% CI (forecast)\"]\n",
    "        ord_h = [by_label[lbl] for lbl in order if lbl in by_label]\n",
    "        ord_l = [lbl for lbl in order if lbl in by_label]\n",
    "        ax.legend(ord_h, ord_l, fontsize=10, loc=legend_loc, framealpha=0.9)\n",
    "\n",
    "    xticks = np.arange(0, plot_len, tick_step)\n",
    "    axs[3].set_xticks(xticks)\n",
    "    axs[3].set_xticklabels(date_labels[xticks], rotation=45, ha='right', fontsize=12)\n",
    "    axs[3].set_xlabel(\"Date\", fontsize=14, fontweight='bold')\n",
    "    plt.suptitle(title, fontsize=18, fontweight='bold', y=0.98)\n",
    "    meta = (f\"Training: {start_date:%Y-%m-%d} to {end_date:%Y-%m-%d} ({train_days} days)\\n\"\n",
    "            f\"Forecast: {forecast_days} days ({(end_date + pd.Timedelta(days=1)):%Y-%m-%d} to {(end_date + pd.Timedelta(days=forecast_days)):%Y-%m-%d})\\n\"\n",
    "            f\"Simulations shown: {len(S_simulations)}\")\n",
    "    fig.text(0.5, 0.91, meta, ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "    if plot_path:\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved: {plot_path}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbadb2",
   "metadata": {},
   "source": [
    "## 7. GŁÓWNA FUNKCJA ANALIZY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88fa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_and_forecast(df, start_date, end_date, forecast_days=21, num_runs=5, use_norm=True, n_particles=NUM_PARTICLES, max_iter=MAX_ITER, dt=DT, population=POPULATION, d_weight=5.0, use_trend_correction=True, use_lhs=True, adaptive_inertia=True, use_islands=True):\n",
    "    dfw = df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)].copy()\n",
    "    dfw.reset_index(drop=True, inplace=True)\n",
    "    days_window = len(dfw)\n",
    "    print(f\"Training window: {start_date.date()} to {end_date.date()} ({days_window} days)\")\n",
    "    print(f\"Forecasting additional {forecast_days} days\")\n",
    "    best_cost = float('inf')\n",
    "    best_params = None\n",
    "    for run_idx in range(num_runs):\n",
    "        print(f\"Running optimization {run_idx+1}/{num_runs}\")\n",
    "        params, history = multi_phase_pso(df, start_date, end_date, forecast_days, use_lhs, adaptive_inertia, use_islands, population, dt, d_weight=d_weight, use_norm=use_norm)\n",
    "        final_cost = history[-1] if history else float('inf')\n",
    "        if final_cost < best_cost:\n",
    "            best_cost = final_cost\n",
    "            best_params = params\n",
    "    print(f\"Best cost: {best_cost:.8f}\")\n",
    "    if best_params is None:\n",
    "        print(\"Ostrzeżenie: Nie znaleziono najlepszych parametrów.\")\n",
    "        return [], {}\n",
    "    print(f\"Best parameters before trend correction: {best_params}\")\n",
    "    if use_trend_correction:\n",
    "        best_params = apply_trend_correction(best_params, df, start_date, end_date, forecast_days)\n",
    "        print(f\"Best parameters after trend correction: {best_params}\")\n",
    "    return [], best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sird_with_advanced_residuals(df, start_date, end_date, forecast_days=21, output_dir=\"results_advanced_residuals\", residual_method=\"markov\", use_trend_correction=True, use_bootstrap=False, use_adaptive_ci=True, n_bootstrap=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Wyniki w: {output_dir}\")\n",
    "    start_str, end_str = start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")\n",
    "    print(\"\\n--- KROK 1: Ulepszone dopasowanie PSO ---\")\n",
    "    _, best_params = fit_and_forecast(df, start_date, end_date, 0, 5, n_particles=20_000, max_iter=150, use_trend_correction=use_trend_correction)\n",
    "    if not best_params:\n",
    "        print(\"Błąd krytyczny: PSO nie zwróciło parametrów.\")\n",
    "        return None\n",
    "    params_path = os.path.join(output_dir, \"params.json\")\n",
    "    try:\n",
    "        with open(params_path, 'w') as f:\n",
    "            json.dump(best_params, f, indent=4)\n",
    "        print(f\"Zapisano parametry: {params_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd zapisu parametrów: {e}\")\n",
    "    print(f\"Najlepsze parametry: {best_params}\")\n",
    "\n",
    "    bootstrap_params = None\n",
    "    if use_bootstrap:\n",
    "        print(\"\\n--- KROK 1b: Bootstrapping parametrów ---\")\n",
    "        bootstrap_params = bootstrap_fit_parameters(df, start_date, end_date, n_bootstrap, 0.8, True)\n",
    "        bootstrap_path = os.path.join(output_dir, \"bootstrap_params.json\")\n",
    "        try:\n",
    "            valid_bs_params = [p for p in bootstrap_params if p]\n",
    "            with open(bootstrap_path, 'w') as f:\n",
    "                json.dump(valid_bs_params, f, indent=4)\n",
    "            print(f\"Zapisano parametry bootstrap: {bootstrap_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd zapisu parametrów bootstrap: {e}\")\n",
    "\n",
    "    print(\"\\n--- KROK 2: Generowanie symulacji stochastycznych ---\")\n",
    "    S_comb, I_comb, R_comb, D_comb = [], [], [], []\n",
    "    if use_bootstrap and bootstrap_params and len([p for p in bootstrap_params if p]) >= 5:\n",
    "        print(\"  Generowanie symulacji z KDE...\")\n",
    "        n_kde_sims = 10_000\n",
    "        diverse_params = generate_diverse_parameters_with_kde(bootstrap_params, n_kde_sims)\n",
    "        if diverse_params:\n",
    "            first_row_df = df[df[\"Last_Update\"] >= start_date].sort_values(\"Last_Update\")\n",
    "            if not first_row_df.empty:\n",
    "                fr = first_row_df.iloc[0]\n",
    "                act_start = fr[\"Last_Update\"]\n",
    "                S0k = POPULATION - (fr[\"Active\"]+fr[\"Recovered\"]+fr[\"Deaths\"])\n",
    "                I0k = fr[\"Active\"]\n",
    "                R0k = fr[\"Recovered\"]\n",
    "                D0k = fr[\"Deaths\"]\n",
    "                days_k = len(df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)]) + forecast_days\n",
    "                fc_start_k = len(df[(df[\"Last_Update\"] >= start_date) & (df[\"Last_Update\"] <= end_date)])\n",
    "                for i, p in enumerate(diverse_params):\n",
    "                    if i % 1000 == 0 and i > 0:\n",
    "                        print(f\"  Wygenerowano {i}/{len(diverse_params)} symulacji z KDE\")\n",
    "                    S_s, I_s, R_s, D_s = enhanced_simulate_sird_rk4(p, days_k, S0k, I0k, R0k, D0k, forecast_start_day=fc_start_k, start_date=act_start)\n",
    "                    S_comb.append(S_s)\n",
    "                    I_comb.append(I_s)\n",
    "                    R_comb.append(R_s)\n",
    "                    D_comb.append(D_s)\n",
    "            else:\n",
    "                print(\"Brak danych do inicjalizacji symulacji KDE.\")\n",
    "        else:\n",
    "            print(\"Nie udało się wygenerować parametrów z KDE.\")\n",
    "\n",
    "    print(\"  Generowanie standardowych symulacji...\")\n",
    "    n_std_sims = 20000\n",
    "    S_s, I_s, R_s, D_s = generate_improved_simulations(best_params, df, start_date, end_date, forecast_days, n_std_sims)\n",
    "    S_comb.extend(S_s)\n",
    "    I_comb.extend(I_s)\n",
    "    R_comb.extend(R_s)\n",
    "    D_comb.extend(D_s)\n",
    "    if not S_comb:\n",
    "        print(\"Błąd krytyczny: Brak symulacji.\")\n",
    "        return None\n",
    "\n",
    "    print(\"  Filtrowanie połączonych trajektorii...\")\n",
    "    S_filt, I_filt, R_filt, D_filt = filter_trajectories(S_comb, I_comb, R_comb, D_comb, df, start_date, end_date, 80)\n",
    "    if not S_filt:\n",
    "        print(\"Błąd krytyczny: Brak symulacji po filtrowaniu.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n--- KROK 3: Korekcja residuów ({residual_method.upper()}) ---\")\n",
    "    if residual_method == \"transformer\":\n",
    "        S_corr, I_corr, R_corr, D_corr = apply_transformer_residuals_correction(S_filt, I_filt, R_filt, D_filt, df, start_date, end_date, forecast_days)\n",
    "    elif residual_method == \"transformer_markov\":\n",
    "        S_corr, I_corr, R_corr, D_corr = apply_transformer_markov_hybrid_correction(S_filt, I_filt, R_filt, D_filt, df, start_date, end_date, forecast_days, 0.8)\n",
    "    else:\n",
    "        S_corr, I_corr, R_corr, D_corr = apply_enhanced_markov_residuals_correction(S_filt, I_filt, R_filt, D_filt, df, start_date, end_date, forecast_days)\n",
    "\n",
    "    print(\"\\n--- KROK 4: Wizualizacja wyników ---\")\n",
    "    if use_adaptive_ci:\n",
    "        print(\"  Tworzenie adaptacyjnych CI...\")\n",
    "        ensemble_res = create_adaptive_confidence_intervals(S_corr, I_corr, R_corr, D_corr, df, start_date, end_date, 700, [50, 80, 95])\n",
    "    else:\n",
    "        print(\"  Tworzenie standardowych CI...\")\n",
    "        ensemble_res = create_weighted_ensemble_with_wider_intervals(S_corr, I_corr, R_corr, D_corr, df, start_date, end_date, 700, [50, 80, 95], 1.25, True)\n",
    "    if not ensemble_res or 'S' not in ensemble_res:\n",
    "        print(\"Błąd krytyczny: Nie utworzono wyników zespołu.\")\n",
    "        return None\n",
    "\n",
    "    print(\"  Korekcja granic CI...\")\n",
    "    corrected_res = apply_data_driven_boundary_correction(ensemble_res, df, start_date, end_date, forecast_days)\n",
    "    print(\"  Egzekwowanie ograniczeń...\")\n",
    "    final_res = enforce_compartment_constraints(corrected_res)\n",
    "    plot_fname = f\"forecast_{residual_method}_{start_str}_to_{end_str}.png\"\n",
    "    plot_fpath = os.path.join(output_dir, plot_fname)\n",
    "    print(f\"  Generowanie wykresu: {plot_fname}\")\n",
    "    fig = plot_stochastic_results(\n",
    "        S_simulations=S_corr,\n",
    "        I_simulations=I_corr,\n",
    "        R_simulations=R_corr,\n",
    "        D_simulations=D_corr,\n",
    "        df=df,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        forecast_days=forecast_days,\n",
    "        title=f\"Prognoza SIRD ({residual_method.upper()}) {start_str} - {end_str}\",\n",
    "        confidence_levels=[50, 80, 95], \n",
    "        ensemble_results=final_res,     \n",
    "        plot_path=plot_fpath            \n",
    "\n",
    "    )\n",
    "    if fig is None:\n",
    "        print(\"Ostrzeżenie: Nie wygenerowano wykresu.\")\n",
    "\n",
    "    return {\"best_params\": best_params, \"simulations\": {\"S\": S_corr, \"I\": I_corr, \"R\": R_corr, \"D\": D_corr}, \"figures\": {\"main\": fig}, \"results\": final_res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Ulepszony Model SIRD z Redukcją Residuów ===\")\n",
    "df = load_covid_data(\"/kaggle/input/polish-cov19-dataset/preprocessed_Poland.csv\")\n",
    "start_date = pd.to_datetime(\"2021-04-04\")\n",
    "end_date = pd.to_datetime(\"2021-05-08\")\n",
    "forecast_days = 21\n",
    "output_dir = f\"sird_analysis_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "residual_method_choice = \"markov\"\n",
    "\n",
    "print(f\"\\nParametry analizy:\")\n",
    "print(f\"- Okres treningowy: {start_date.date()} do {end_date.date()}\")\n",
    "print(f\"- Dni prognozy: {forecast_days}\")\n",
    "print(f\"- Metoda residuów: {residual_method_choice}\")\n",
    "print(f\"- Katalog wyjściowy: {output_dir}\")\n",
    "\n",
    "results = run_sird_with_advanced_residuals(\n",
    "         df=df, start_date=start_date, end_date=end_date, forecast_days=forecast_days,\n",
    "         residual_method=residual_method_choice, output_dir=output_dir, n_bootstrap=10\n",
    "     )\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nAnaliza zakończona pomyślnie. Wyniki zapisano w katalogu: {output_dir}\")\n",
    "else:\n",
    "    print(\"\\nAnaliza zakończona z błędami.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fa6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6791595,
     "sourceId": 10923840,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 945.211461,
   "end_time": "2025-03-30T22:07:39.133678",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-30T21:51:53.922217",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
